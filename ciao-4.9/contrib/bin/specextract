#!/usr/bin/env python

#
# Copyright (C) 2011, 2013, 2014 Smithsonian Astrophysical Observatory
#
#
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
#

"""
Script:

  specextract 

This script is an enhanced Python translation of the original CIAO tool specextract, which was written in S-Lang. It is run from the Unix command line, within the CIAO environment.

"""

__version__ = "CIAO 4.9"

toolname = "specextract"
__revision__ = "30 January 2017"


##########################################################################
#
# This script requires a parameter file for itself and for its underlying
# tools:  dmextract, mkrmf, mkacisrmf, mkwarf, mkarf, sky2tdet, acis_fef_lookup,
# dmgroup, dmhedit, calquiz, asphist, and acis_set_ardlib.
# 
##########################################################################

# Load the necessary libraries.

import paramio, os, sys, shutil, numpy, string, logging, cxcdm, time, tempfile
import stk as st
import pycrates as pcr

from ciao_contrib.logger_wrapper import initialize_logger, make_verbose_level, set_verbosity, handle_ciao_errors, get_verbosity
from ciao_contrib.param_wrapper import open_param_file
from ciao_contrib.runtool import dmextract, mkrmf, mkacisrmf, mkwarf, dmgroup, dmhedit, calquiz, dmcopy, dmstat, acis_fef_lookup, mkarf, asphist, sky2tdet, arfcorr, combine_spectra, ardlib, acis_set_ardlib, dmmakereg, dmlist, dmkeypar, dmcoords, stk_build, stk_read_num, stk_count, add_tool_history
from ciao_contrib.cxcdm_wrapper import get_block_info_from_file, get_info_from_file 
import ciao_contrib.cxcdm_wrapper as dmw 
from ciao_contrib.cxcdm_wrapper import open_block_from_file, close_block
from crates_contrib.utils import write_columns
from ciao_contrib.stacklib import make_stackfile
import ciao_contrib._tools.fileio as fileio
import ciao_contrib._tools.utils as utils
import ciao_contrib._tools.obsinfo as obsinfo
from ciao_contrib.proptools import colden

###############################################


# Set up the logging/verbose code
initialize_logger(toolname)

# Use v<n> to display messages at the given verbose level.
v0 = make_verbose_level(toolname, 0)
v1 = make_verbose_level(toolname, 1)
v2 = make_verbose_level(toolname, 2)
v3 = make_verbose_level(toolname, 3)
v4 = make_verbose_level(toolname, 4)
v5 = make_verbose_level(toolname, 5)


#########################################################################################
#
# suppress warnings printed to screen from get_keyvals when probing for blank sky files
# http://stackoverflow.com/questions/11130156/suppress-stdout-stderr-print-from-python-functions
#
#########################################################################################
class suppress_stdout_stderr(object):
    '''
    A context manager for doing a "deep suppression" of stdout and stderr in 
    Python, i.e. will suppress all print, even if the print originates in a 
    compiled C/Fortran sub-function.
       This will not suppress raised exceptions, since exceptions are printed
    to stderr just before a script exits, and after the context manager has
    exited (at least, I think that is why it lets exceptions through).      

    '''
    def __init__(self):
        # Open a pair of null files
        self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]
        # Save the actual stdout (1) and stderr (2) file descriptors.
        self.save_fds = (os.dup(1), os.dup(2))

    def __enter__(self):
        # Assign the null pointers to stdout and stderr.
        os.dup2(self.null_fds[0],1)
        os.dup2(self.null_fds[1],2)

    def __exit__(self, *_):
        # Re-assign the real stdout/stderr back to (1) and (2)
        os.dup2(self.save_fds[0],1)
        os.dup2(self.save_fds[1],2)
        # Close the null files
        os.close(self.null_fds[0])
        os.close(self.null_fds[1])


#############################################################################
# This function was retained from the original specextract - it may come in
# handy for future RFEs.
#############################################################################

def docommand(cmd):

    if 0 == os.system(cmd):
        return 0
    else:
        v0(cmd + "  Failed.")
        return 1
  
################################################################################
# Usage:
#  check_files(stack, filetype)
#
# Aim:
#  Checks to see that each file in an input stack is readable (also catches
#   the case of a non-existent file, e.g., the user misspelled the filename).
#
################################################################################

def check_files(stack, filetype):
    """Check to see if each file in the input stack is readable."""

    count = len(stack)
    if count == 1:
        suffix = ''
    else:
        suffix = 's'

    v2("Checking {0} file{1} for readability...".format(filetype, suffix))

    for s in stack:
        # str() should not be needed, but left in, just in case
        filename = str(s)
        v2("  validating file: {}".format(filename))
        if filename in ["CALDB", "None", "None", "none"]:
            v3("  ... skipping")
            continue

        v3("  ... checking if it exists")
        try:
            table = cxcdm.dmTableOpen(filename)

        except IOError:
            raise IOError("{0} file {1} does not exist or could not be opened.".format(filetype,filename))
                    
        cxcdm.dmTableClose(table)  
        
################################################################################
# Usage:
#  check_file_pbkheader(infile_stack)
#
# Aim:
#  Checks that the event files in a stack contain the PBK and ASOL-derived header
#  keywords introduced in ReproIV
#
################################################################################
        
def check_file_pbkheader(infile_stack):
    """check the input file header contains keywords that were found in the pbk file and derived from the asol"""

    ## pbkkeys are: 'OCLKPAIR', 'ORC_MODE', 'SUM_2X2', 'FEP_CCD'
    ## asol-derived keys are: 'DY_AVG', 'DZ_AVG', 'DTH_AVG' 

    pbk_kw = ("OCLKPAIR","ORC_MODE","SUM_2X2","FEP_CCD")
    #asp_kw = ["DY_AVG","DZ_AVG","DTH_AVG"]

    file_status = []

    for inf in infile_stack:
        inf = fileio.get_file(inf)

        headerkeys = fileio.get_keys_from_file(inf)

        #verify_keys = [headerkeys.has_key(kw) for kw in asp_kw] # P2
        #verify_keys = [] # P2
        #
        # if headerkeys["INSTRUME"] == "ACIS": # P2
        #     verify_keys.extend([headerkeys.has_key(kw) for kw in pbk_kw]) # P2
        #
        # if False in verify_keys: # P2
        #     v1("WARNING: {} missing header keywords.\n".format(fileio.remove_path(inf))) # P2
        #
        #     file_status.append(inf) # P2

        ### P3 ###
        if headerkeys["INSTRUME"] == "ACIS":
            try:
                verify_keys = []

                verify_keys.extend([headerkeys.has_key(kw) for kw in pbk_kw])

                if False in verify_keys:
                    v1("WARNING: {} missing header keywords.\n".format(fileio.remove_path(inf)))

                    file_status.append(inf)

            except AttributeError:
                pbk_kw_list = list(pbk_kw)

                verify_keys = headerkeys.keys() & pbk_kw_list # find elements in common

                for s in verify_keys:
                    pbk_kw_list.remove(s)

                if len(pbk_kw_list) != 0:
                    v1("WARNING: {} missing header keywords.\n".format(fileio.remove_path(inf)))

                    file_status.append(inf)
        ### P3 ###

    if file_status != []:
        raise IOError("Input event file(s) missing necessary Repro IV header keywords.  Reprocess data with chandra_repro or add the keywords to the event file(s) with r4_header_update.")

##########################################################################
# Usage:
#  extract_spectra( full_outroot, infile, ptype, ewmap,
#                   binwmap, instrument, clobber, verbose)
#
# Aim:
#   Create spectrum from input file.
#   
##########################################################################

def extract_spectra(full_outroot, infile, ptype, ewmap, binwmap, instrument, clobber, verbose):
    # output spectrum filename
    specfile = "{0}.{1}".format(full_outroot,ptype.lower())

    dmextract.punlearn() 
    dmextract.infile  =  "{0}[bin {1}]".format(infile,ptype)
    dmextract.outfile =  specfile 
    dmextract.opt     =  "pha1"
    dmextract.clobber =  clobber
    dmextract.verbose =  str(verbose)

    if instrument == "ACIS":
        dmextract.wmap = "[energy={0}][bin {1}]".format(ewmap,binwmap)
    else:
        dmextract.wmap = ""
    
    return dmextract(), specfile


##########################################################################
# Usage:
#   build_rmf_ext(rmftool, ptype, full_outroot, ebin,
#              rmfbin, clobber, verbose, specfile, weightfile)
#
#
# Aim:
#   Run either mkrmf or mkacisrmf depending on input conditions.
#   For mkrmf:  input CALDB and weight file and return RMF name.
#   For mkacisrmf:  input CALDB and WMAP and return RMF name.
#   
##########################################################################

def build_rmf_ext(rmftool, ptype, full_outroot, ebin, rmfbin, clobber, verbose, specfile, weightfile):
  
    # RMF filename to return
    #rmffile = full_outroot + ".wrmf"
    rmffile = full_outroot + ".rmf"

    if "mkrmf" == rmftool:
  
        mkrmf.punlearn()
     
        mkrmf.infile  =  "CALDB"
        mkrmf.outfile =  rmffile
        mkrmf.logfile =  ""
        mkrmf.weights =  weightfile
        mkrmf.axis1   =  "energy={}".format(ebin) 
        mkrmf.axis2   =  rmfbin 
        mkrmf.clobber =  clobber
        mkrmf.verbose =  str(verbose)

        return mkrmf(), rmffile
  
    elif "mkacisrmf" == rmftool:

        channel = rmfbin.split("=")  
        channel.reverse()
        
        mkacisrmf.punlearn()
      
        mkacisrmf.infile   =  "CALDB"
        mkacisrmf.outfile  =  rmffile
        mkacisrmf.energy   =  ebin 
        mkacisrmf.channel  =  channel[0] 
        mkacisrmf.chantype =  ptype.upper() 
        mkacisrmf.wmap     =  "{}[WMAP]".format(specfile)
                              #dmextract WMAP is faster than sky2tdet WMAP
        mkacisrmf.gain     =  "CALDB"
        mkacisrmf.clobber  =  clobber 
        mkacisrmf.verbose  =  str(verbose)
        mkacisrmf.ccd_id   =  "0" # hopefully, this par will
                                  # be ignored since it can't
                                  # be set to "".

        return mkacisrmf(), rmffile
  

##########################################################################
# Usage:
#   build_rmf_ps(rmftool, ptype, full_outroot, ebin,
#              rmfbin, clobber, verbose, specfile, weightfile, ccd_id, chipx, chipy)
#
#
# Aim:
#   Run either mkrmf or mkacisrmf depending on input conditions.
#   For mkrmf:  input FEF file and no weight file and return RMF name.
#   For mkacisrmf:  input CALDB and WMAP and return RMF name.
#   
##########################################################################

def build_rmf_ps(rmftool, evt_filename, infile, ptype, full_outroot, ebin, rmfbin, clobber, verbose, specfile, weightfile, ccd_id, chipx, chipy):

    # RMF filename to return
    rmffile = full_outroot + ".rmf"

    if "mkrmf" == rmftool:
  
        acis_fef_lookup.punlearn()
        
        acis_fef_lookup.infile = evt_filename
        acis_fef_lookup.chipid = ccd_id
        acis_fef_lookup.chipx= chipx
        acis_fef_lookup.chipy = chipy
        acis_fef_lookup.verbose = str(0)
        acis_fef_lookup()

        mkrmf.punlearn()
     
        mkrmf.infile  =  acis_fef_lookup.outfile
        mkrmf.outfile =  rmffile
        mkrmf.logfile =  ""
        mkrmf.weights =  ""
        mkrmf.axis1   =  "energy={}".format(ebin)
        mkrmf.axis2   =  rmfbin 
        mkrmf.clobber =  clobber
        mkrmf.verbose =  str(verbose)

        return mkrmf(), rmffile
  
    elif "mkacisrmf" == rmftool:

        channel = rmfbin.split("=")
        channel.reverse()
        
        mkacisrmf.punlearn()
        
        # force CALDB querry to match 'ccd_id' parameter, otherwise the default
        # behavior is to use the CCD_ID header keyword in the 'obsfile'
        mkacisrmf.infile   =  "CALDB(CCD_ID={})".format(ccd_id)
        mkacisrmf.outfile  =  rmffile
        mkacisrmf.energy   =  ebin
        mkacisrmf.channel  =  channel[0]
        mkacisrmf.chantype =  ptype.upper()
        mkacisrmf.wmap     =  "none"
        mkacisrmf.gain     =  "CALDB"
        mkacisrmf.obsfile  =  evt_filename
        mkacisrmf.ccd_id   =  ccd_id #event_stats(infile, "ccd_id")
        mkacisrmf.chipx    =  chipx #event_stats(infile, "chipx")
        mkacisrmf.chipy    =  chipy #event_stats(infile, "chipy")
        mkacisrmf.clobber  =  clobber
        mkacisrmf.verbose  =  str(verbose)

        return mkacisrmf(), rmffile


################################################################################
# Usage:
#  check_key(filename, key)
#
# Aim:
#  Reads in the data from the filename and checks for the specified keyword,
#  issuing a warning if one is not found.
#
#################################################################################

def check_key(filename, key):
    
    """Reads in the data from the filename and checks for the specified
    keyword, issuing a warning if one is not found."""
    
    #cr = pcr.read_file(filename)

    #if cr == None:
    #    raise IOError("\nUnable to read data from file %s." % filename)
    #    return 0

    
    #if pcr.key_exists(cr, key) != 1:                    
    #    v1("\nNo %s keyword in header of file '%s'." % (key,filename))
    #    return 0   
       
    #else:   
        
    #    return 1
    
    bl = open_block_from_file(filename)
    try:
        try:
            cxcdm.dmKeyOpen(filename, key)
            rval = True
            
        except ValueError:
            # v1("\nNo {0} keyword in header of file '{1}'.".format(key, filename))
            rval = False

    finally:
        close_block(bl)

    return rval
    

##########################################################################
# Usage:
#  get_keyvals(stack, keyname)
#
# Aim:
#  Needed for input to the dictionary output by group_by_obsid() for
#  matching (TSTART-sorted) input asol files to each input source file.
#  For now, 'stack' should be of the form 'stack=stk_build(files)'.
#
###########################################################################

def get_keyvals(stack, keyname):

    #count = stk_count(stack)
    #keys = range(count)
    
    #for i in range(0, count):
    
    #    if check_key(stk_read_num(stack,i+1), keyname) != 0:
            
    #        info = get_block_info_from_file(stk_read_num(stack, i+1))
    #        key_record = info[1]["records"][keyname]
    #        key_strval = str(key_record).split(",")[1]

        
    #        if "'" in str(key_strval):
    #            key_num = str(key_strval).split("'")[1]
    #        else:
    #            key_num = str(key_strval).split("=")[1]

    #        if not key_num:
    #           key_num="-99"
               
    #        keys[i] = key_num

                
    #    else:
    #        keys[i]="-99"
    #        v1("WARNING: The %s header keyword is missing from %s" % (keyname, stk_read_num(stack,i+1)))
        

    #return keys
    store = {}

    keys = []
    count = len(stack)
    
    for i in range(0, count):
        fname = stack[i] #stk_read_num(stack, i+1)
    
        try:
            kval = store[fname]

        except KeyError:
            kval = get_keyval(fname, keyname, default="-99")
            store[fname] = kval

        keys.append(kval)

    return keys

##########################################################################
# Usage:
#  get_keyval(file, keyname)
#
# Aim:
#  Retrieve file header keyword value.
#
###########################################################################

def get_keyval(file, keyname, default="NONE"):
#def get_keyval(file, keyname):

    #if check_key(file, keyname) != 0:
            
    #    info = get_block_info_from_file(file)
    #    key_record = info[1]["records"][keyname]
    #    key_strval = str(key_record).split(",")[1]

    #    if "'" in str(key_strval):
    #        key = str(key_strval).split("'")[1]
    #    else:
    #        key = str(key_strval).split("=")[1]

    #    if not key or str(key) in ["NONE","None","none",""," ","0.0","0"]:
    #        key="NONE"
    #    else:
    #        key = key
    #else:
    #    key="NONE"
        
    #return key            
    
    bl = open_block_from_file(file)

    try:
        try:
            # dmKeyRead will return the "appropriate" Python type for
            # a keyword, which may make sense to use, but for now
            # we convert back to a string since this is what the
            # original code did.
            #
            (dummy, val) = cxcdm.dmKeyRead(bl, keyname)
            
            # val = str(val) # P2

            try:                    # P3
                val = val.decode()  # P3
    
            except AttributeError:  # P3
                val = str(val)      # P3   

            # Continue to convert empty values (or 0/0.0) to default
            # value, until the script checks for a keyword for which
            # null/zero value is valid.

            if val in ["", "0.0", "0", "None", "none", "NONE"]:
                #v0("NOTE: found a keyword value that used to be converted to NONE but now left as is (key={0})".format(keyname))
                v3("WARNING: found an empty/zero/null {0} keyword value in {1}.\n".format(keyname,file))
                val = default

            elif keyname.upper()=="OBS_ID" and all(char in string.digits for char in val)==False:
                val= default
                    
        except ValueError:
            v1("WARNING: The {0} header keyword is missing from {1}".format(keyname, file))
            val = default

    finally:
        close_block(bl)

    return val
        
##########################################################################
# Usage:
#   set_badpix(evtfile, bpixfile, instrument, verbose)
#
# Aim:
#  Allow the user to set a different bad pixel file in ARDLIB for each
#  observation in an input source or background stack.
#
##########################################################################

def set_badpix(evtfile, bpixfile, instrument, verbose):

    #ardlib.punlearn()
    ardlib.read_params()

    if instrument == "ACIS":
        bpixfile = ",".join(st.build(bpixfile))
        
        acis_set_ardlib.punlearn()

        acis_set_ardlib.badpixfile = bpixfile
        acis_set_ardlib.verbose    = verbose

        acis_set_ardlib()
        ardlib.write_params()

        return acis_set_ardlib()
    
    else:
        bpixpar = "AXAF_{}_BADPIX_FILE".format(get_keyval(evtfile,"detnam"))
        bpix = "{}[BADPIX]".format(bpixfile)

        try:
            setattr(ardlib,bpixpar.replace("-","_"),bpix)
            ardlib.write_params()

        finally:
            if os.path.isfile(bpixfile) and getattr(ardlib,bpixpar.replace("-","_")).rstrip("[BADPIX]") == bpixfile:
                return True
            else:
                return None
            
            
###########################################################################
# Usage:
#  convert_region(infile, evt_filename, clobber, verbose)
#
# Aim:
#  Convert user-input source/background regions to physical coordinates, in
#  order to ensure that regions input to arfcorr via the correct_arf() function
#  are in the correct form. Running this function on regions already in
#  physical coordinates has no effect other than converting CIAO region
#  format to DS9 format.
#  
############################################################################

def convert_region(infile, evt_filename, outfile, clobber, verbose):
    
  # Output region name

    dmmakereg.punlearn()
    
    dmmakereg.region  = get_region_filter(infile)[1] 
    dmmakereg.outfile = outfile 
    dmmakereg.wcsfile = evt_filename
    dmmakereg.kernel  = "fits"
    dmmakereg.clobber = clobber
    dmmakereg.verbose = verbose

    dmmakereg()

    return infile.replace(get_region_filter(infile)[1], "region("+outfile+")")



#########################################################################
# Usage:
#  sort_files(file_stack, file_type, key)
#
#
# Aim:
#  Sort a list of files by the value of a given keyword; e.g., for sorting 
#  a stack of aspect solution files on TSTART before input to mk_asphist().
#
##########################################################################

def sort_files(file_stack, file_type, key):
    
    file_count = len(file_stack)
    files_orig_order = list(range(file_count))

    for i in range(0, file_count):
        files_orig_order[i] = file_stack[i] #stk_read_num(file_stack, i+1)
    

    keyvals_orig_order = get_keyvals(file_stack, key)

    if "-99" in keyvals_orig_order:
        raise IOError("One or more of the entered {0} files is missing the required {1} header keyword value. Exiting.".format(file_type,key))


    keyvals_sorted = sorted(keyvals_orig_order) #be sure numbers are not strings
    
    files_sorted  = list(range(file_count))
           
    for i in range(0, file_count):
        sort_ind = keyvals_orig_order.index(keyvals_sorted[i])
        files_sorted[i] = files_orig_order[sort_ind]

    return files_sorted

##########################################################################
# Usage:
#  group_by_obsid(file_stack, file_type)
#
#
# Aim:
#  Read the OBS_ID value from the header of each file in the input stack
#  in order to create a dictionary matching each ObsID to one or a list of
#  files. Then, this dictionary can be used by other functions in the script
#  to assign the appropriate file(s) (e.g., asol) to each source observation,
#  by obsid.
#
##########################################################################

def group_by_obsid(file_stack, file_type):

    # Right now, for simplicity, accept stacks
    # defined as follows:
    #
    # file_stack = st.stack_build(files)
    #
    # where 'files' is a list or @stack read from
    # specextract parameter file.
 
    file_count = len(file_stack)
    
    obsids  = get_keyvals(file_stack, "OBS_ID")

    # //// NEW 19April2012 ////////////////////////////////
    #badstring=[]
    #for obsid in obsids:
    #    badstring.append(isinstance(obsid, basestring))
    # ///////////////////////////////////////////////////// 


    if "-99" in obsids:
        raise IOError("One or more of the entered {} files is missing an OBS_ID header keyword value or it exists but contains an unexpected value, therefore files cannot be grouped by ObsID and properly matched to input source files. Exiting.".format(file_type))
       # can be just a warning for certain file types, but right now this
       # function is just used for asol


    # //// NEW 19April2012 ////////////////////////////////
    #elif True in badstring:
    #    raise IOError("One or more of the entered %s files contains an unexpected OBS_ID header keyword value, 
    #    therefore files cannot be grouped by ObsID and properly matched to input source files. Exiting." % (file_type))
    # ///////////////////////////////////////////////////// 

    obsid_sort = sorted(obsids) # be sure numbers are not strings

    orig_files = list(range(file_count))
    for i in range(0, file_count):
        orig_files[i] = file_stack[i] #stk_read_num(file_stack, i+1)

    sorted_files  = list(range(file_count))
    for i in range(0, file_count):
        sort_ind = obsids.index(obsid_sort[i])
        sorted_files[i] = orig_files[sort_ind]
             

    if file_type != "aspsol":

        obsid_dict={}
        for i in range(0, file_count):
            if obsid_sort[i] in obsid_dict:
                obsid_dict[obsid_sort[i]] = "{0},{1}".format(obsid_dict[obsid_sort[i]],sorted_files[i])
            else:
                obsid_dict[obsid_sort[i]] = sorted_files[i]

        return obsid_dict

    else:        
        asol_tstart = get_keyvals(file_stack, "TSTART")

        if "-99" in asol_tstart:
            raise IOError("One or more of the entered aspect solution files is missing a TSTART header keyword value, therefore files cannot be properly sorted and matched to input source files. Exiting.")

        asol_tstart_sort = sorted(asol_tstart) #be sure numbers are not strings

        sort_asolfiles  = list(range(file_count))
        asol_obsid_sort = list(range(file_count))
        for i in range(0, file_count):
            tsort_ind = asol_tstart.index(asol_tstart_sort[i])
            sort_asolfiles[i] = orig_files[tsort_ind]
            asol_obsid_sort[i] = obsids[tsort_ind] 

        asol_dict={}
        for i in range(0, file_count):
            if asol_obsid_sort[i] in asol_dict:

                asol_sort = "{0},{1}".format(asol_dict[asol_obsid_sort[i]],sort_asolfiles[i]).replace(" ","").split(",")
                unique_asol_sort = ",".join(utils.getUniqueSynset(asol_sort))

                if len(asol_sort) != len(unique_asol_sort):
                    v3("WARNING: Duplicate aspect solutions provided.")
                
                asol_dict[asol_obsid_sort[i]] = unique_asol_sort
            else:
                asol_dict[asol_obsid_sort[i]] = sort_asolfiles[i]

        return asol_dict    
        

##########################################################################
# Usage:
#  mk_asphist(asol_param, evt_filename, infile, full_outroot, instrument, dtf, chip_id, verbose, clobber)
#
#
# Aim:
#   Run asphist to create one aspect histogram file per user-input source
#   observation, for input to sky2tdet ('weight=yes') or mkarf ('weight=no');
#   the assumption is that it is appropriate to analyze each observation using
#   only one, and not multiple, aspect histogram files.
#
##########################################################################

def mk_asphist(asol_param, evt_filename_filter, full_outroot, dtffile, instrument, chip_id, verbose, clobber, tmpdir):

    #asp_out = "{0}_asphist{1}.fits".format(full_outroot,chip_id) #full_outroot+"_asphist"+str(event_stats(infile, "ccd_id"))+".fits"
    asp_out = tempfile.NamedTemporaryFile(suffix="_asphist{}".format(chip_id),dir=tmpdir)
                
    asphist.punlearn()
   
    asphist.infile  = asol_param  # single file, @files.lis, or
                                  # comma-separated list of files
                                              
    if instrument == "ACIS":
        asphist.evtfile = "{0}[ccd_id={1}]".format(evt_filename_filter,chip_id) #evt_filename+"[ccd_id="+str(event_stats(infile, "ccd_id"))+"]"
        asphist.dtffile = ""
    else:
        asphist.evtfile = "{0}[chip_id={1}]".format(evt_filename_filter,chip_id) #evt_filename+"[chip_id="+str(event_stats(infile, "chip_id"))+"]"
        asphist.dtffile = dtffile

    asphist.outfile = asp_out.name
    asphist.verbose = verbose
    asphist.clobber = "yes"
    
    return asphist(), asp_out.name, asp_out


##########################################################################
# Usage:
#  create_arf_ext( outtype, outroot, ebin, clobber, verbose, 
#              specfile, pbkfile, dafile)
#
#
# Aim:
#   Run mkwarf to create a weighted ARF.  Input the sky2tdet WMAP to create
#   an ARF and weight file (to be used for creating a RMF, if using mkrmf).
#   Return the name of the ARF.
#   
##########################################################################

#def create_arf_ext(full_outroot, infile, asp_param, ebin, clobber, verbose, specfile, pbkfile, dafile, mskfile, ewmap_param, bintwmap_param, pars):
def create_arf_ext(full_outroot, infile, asp_param, ebin, clobber, verbose, specfile, dafile, mskfile, ewmap_param, bintwmap_param, pars, tmpdir):

    # output TDET WMAP filename
    #tdetwmap = full_outroot + "_" + "tdet.fits[wmap]"
    tdetwmap = tempfile.NamedTemporaryFile(suffix="_tdet",dir=tmpdir)

    try:
        sky2tdet.punlearn()

        sky2tdet.infile      = "{0}[energy={1}][bin sky]".format(infile,ewmap_param)
                               # include extraction region
                               # plus same energy filter
                               # used for dmextract wmap input to mkacisrmf
        sky2tdet.bin         = bintwmap_param
        sky2tdet.asphistfile = asp_param
        sky2tdet.outfile     = "{}[wmap]".format(tdetwmap.name)
        sky2tdet.clobber     = "yes"
        sky2tdet.verbose     = verbose #"0"

        sky2tdet()
        #add_tool_history(tdetwmap.name,toolname,pars,toolversion=__revision__)

        #++++++++++++++++++++++++++++++++++++++++++++
        # NO LONGER NEEDED IN CIAO 4.4 AND ON.
        # Remove extraneous time header keywords from
        # output of sky2tdet before input to mkwarf.

        #ov = get_verbosity()
        #set_verbosity(0)
        #dmw.remove_extra_time_keywords(tdetwmap)
        #set_verbosity(ov) 
        # ++++++++++++++++++++++++++++++++++++++++++

        # ARF output file
        #arffile = full_outroot + ".warf"
        arffile = full_outroot + ".arf"

        # output weight file used in mkrmf
        #weightfile = full_outroot + ".wfef"
        weightfile = tempfile.NamedTemporaryFile(suffix=".wfef",dir=tmpdir)

        mkwarf.punlearn()

        mkwarf.infile       =  "{}[wmap]".format(tdetwmap.name)
        mkwarf.outfile      =  arffile
        mkwarf.weightfile   =  weightfile.name 
        #mkwarf.pbkfile      =  pbkfile
        mkwarf.mskfile      =  mskfile
        mkwarf.dafile       =  dafile 
        mkwarf.spectrumfile =  ""
        mkwarf.egridspec    =  ebin
        mkwarf.clobber      =  "yes"
        mkwarf.verbose      =  str(verbose) 

        ret = mkwarf()

    #except IOError, err_msg: 
    except IOError as err_msg: # P3
        print(err_msg.message)

        raise IOError("Failure to create weighted ARF.  Possible causes include: zero counts in the input region; a memory allocation error; or corrupt ARDLIB.  Try running {} with weight=no, binarfmap!=1, or punlearn ardlib.".format(toolname))

    tdetwmap.close()
    
    return ret, arffile, weightfile.name, weightfile


###############################################################
# Usage:
#  check_event_stats(file)
#
#
# Aim:
#     Use dmlist to determine the event counts
#     and appropriately error out or proceed with the script
#
################################################################     

def check_event_stats(file,refcoord,weights_check):     

    dmlist.punlearn()
    dmlist.infile = file 
    dmlist.opt = "counts"
   
    if dmlist() == "0":
        if refcoord.lower() not in ["","none","indef"]:
            if weights_check == True:
                v1("WARNING: Unweighted responses will be created at the refcoord position.\n")
            else:
                v1("WARNING: Using refcoord position to produce response files.\n")
        else:
            raise IOError("{0} has zero counts. Check that the region format is in sky pixels coordinates.".format(file)) 

        return False

    else:
        return True


def create_hrc_resp(specfile,refcoord,full_outroot,asp_param,clobber,verbose,mskfile,skyx,skyy,instrument,chip_id):

    # check number of channels in PI file to determine RMF file to use from CalDB
    dmlist.punlearn()
    dmlist.infile = specfile
    dmlist.opt = "counts"

    chan = int(dmlist())
    
    detector = get_keyval(specfile,"detnam")

    # cf. http://cxc.cfa.harvard.edu/cal/Hrc/detailed_info.html#rmf for HRC RMF information
    #
    # get path from $CALDB environmental variable
    CALDB = os.environ["CALDB"]
    
    if chan == 1024:
        # use SAMP responses
        if detector.upper() == "HRC-S":
            rmf = "{}/data/chandra/hrc/rmf/hrcsD1999-07-22samprmfN0001.fits".format(CALDB)
        elif detector.upper() == "HRC-I":
            rmf = "{}/data/chandra/hrc/rmf/hrciD1999-07-22samprmfN0001.fits".format(CALDB)        
        else:
            raise ValueError("{0} has an invalid HRC detnam value.".format(specfile))
        
    elif chan == 256:
        # use non-SAMP responses
        if detector.upper() == "HRC-S":
            rmf = "{}/data/chandra/hrc/rmf/hrcsD1999-07-22rmfN0001.fits".format(CALDB)
        elif detector.upper() == "HRC-I":
            rmf = "{}/data/chandra/hrc/rmf/hrciD1999-07-22rmfN0002.fits".format(CALDB)
        else:
            raise ValueError("{0} has an invalid HRC detnam value.".format(specfile))
        
    else:
        raise IOError("{0} is an invalid HRC spectral file".format(specfile))

    # if specfile fails for obsfile, introduce use of event file, which adds another variable to this function

    # copy RMF
    rmffile = full_outroot + ".rmf"

    # use opt=all to get the ebounds block in addition to the specresp matrix block
    #dmcopy.punlearn()
    #dmcopy.infile = rmf
    #dmcopy.outfile = rmffile
    #dmcopy.option = "all"
    #dmcopy.clobber = clobber
    #dmcopy.verbose = verbose
    #dmcopy()
    shutil.copyfile(rmf,rmffile)

    # establish detsubsys
    if instrument == "HRC":
        if chip_id == "0":
            detname = "HRC-I"
        else:
            detname = "HRC-S{}".format(chip_id)

    # ARF output file
    arffile = full_outroot + ".arf"

    mkarf.punlearn()
    
    mkarf.detsubsys     = detname
    mkarf.outfile       = arffile  
    mkarf.asphistfile   = asp_param
    mkarf.sourcepixelx  = skyx
    mkarf.sourcepixely  = skyy
    mkarf.grating       = get_keyval(specfile,"GRATING") 
    mkarf.obsfile       = specfile
    mkarf.maskfile      = mskfile      
    mkarf.verbose       = str(verbose)
    mkarf.engrid        = "grid({0}[SPECRESP MATRIX][cols ENERG_LO,ENERG_HI])".format(rmffile)
    mkarf.clobber       = clobber

    return mkarf(), arffile, rmffile
        

def resp_pos(infile,asol,refcoord):

    dmcoords.punlearn()
    dmcoords.infile = get_filename(infile)
    dmcoords.asolfile = asol
    dmcoords.celfmt = "deg"

    if refcoord != "":
        # returns RA and Dec in decimal degrees
        ra,dec,delme = utils.parse_refpos(refcoord.replace(","," "))

        dmcoords.opt = "cel"
        dmcoords.ra = str(ra)
        dmcoords.dec = str(dec)

        dmcoords()

        skyx = str(dmcoords.x)
        skyy = str(dmcoords.y)

    else:
        ## parse infile region
        if get_region(infile) == infile:
            raise IOError("No region filter with the event file, nor a refcoords value, provided to produce response files.")

        else:
            # follow general procedures used in event_stats()

            dmstat.punlearn()
            dmstat.verbose = "0"
            dmstat.centroid = "yes"
            dmstat("{}[bin sky=2]".format(infile))
            
            # get sky position
            #skyx,skyy = dmstat.out_max_loc.split(",") ## since for an event file, we want the out_mean_loc, which for an image will correspond to the out_cntrd_phys value, which is consistent with documentation, although there may be a preference for using the maximum value instead.  

            #skyx,skyy = dmstat.out_cntrd_phys.split(",") #having issues if centroid falls in a zero-count location
            skyx,skyy = dmstat.out_max_loc.split(",")

            # convert to chip coordinates
            dmcoords.opt = "sky"
            dmcoords.x = str(skyx)
            dmcoords.y = str(skyy)
            
            dmcoords()

    chipx = str(int(dmcoords.chipx))
    chipy = str(int(dmcoords.chipy))

    chip_id = str(dmcoords.chip_id)

    ra = float(dmcoords.ra)
    dec = float(dmcoords.dec)

    return ra,dec,skyx,skyy,chipx,chipy,chip_id

###############################################################
# Usage:
#  event_stats(file, colname)
#
#
# Aim:
#     Use dmstat to determine the event statistics: source chipx,
#     chipy, sky x, sky y, and ccd_id values to use for input
#     to asphist, acis_fef_lookup, and arfcorr.
#     Since dmstat doesn't return the mode, do this by brute force
#     for ccd_id column (psextract algorithm uses mean ccd_id value,
#     which isn't appropriate for this quantity).
#
################################################################     

def event_stats(file, colname):     

##     dmlist.punlearn()
##     dmlist.infile = file 
##     dmlist.opt = "counts"
##   
##     if dmlist()=="0":    
##         raise IOError("{0} has zero counts. Check that the region format is correct.".format(file)) 

    dmstat.punlearn()
    dmstat.verbose="0"
        
    if colname=="ccd_id":

      # Use pycrates to retrieve ccd_id values from user-input event
      # extraction region. Compute the mode of the ccd_id values
      # stored in the array.
     
      cr = pcr.read_file(file)
      if cr is None:
          raise IOError("Unable to read from file {}".format(file))
      
      
      ccdid_vals = pcr.get_colvals(cr, "ccd_id")
    
      n_elements = list(range(10))
      
      for i in range(0,10):
          zeros = numpy.array(ccdid_vals)-i
          n_elements[i] = len(ccdid_vals)-len(numpy.nonzero(zeros)[0])
           
      mode = numpy.where(n_elements==numpy.max(n_elements))[0]
      return mode[0]

    elif colname == "chip_id":

        # Use pycrates to retrieve ccd_id values from user-input event
        # extraction region. Compute the mode of the ccd_id values
        # stored in the array.
     
        cr = pcr.read_file(file)
        if cr is None:
            raise IOError("Unable to read from file {}".format(file))
      
        chipid_vals = pcr.get_colvals(cr, "chip_id")
    
        n_elements = list(range(10))
        
        for i in range(0,10):
            zeros = numpy.array(chipid_vals)-i
            n_elements[i] = len(chipid_vals)-len(numpy.nonzero(zeros)[0])
           
        mode = numpy.where(n_elements==numpy.max(n_elements))[0]
        return mode[0]

    elif str(colname) in ["x","y","chipx","chipy"]:
            
        if colname=="x" or colname=="y":
            bin_setting="[bin sky=2]"  
 
        elif colname=="chipx" or colname=="chipy":
            bin_setting="[bin chipx=2,chipy=2]" 
 
        dmstat(file+bin_setting)

        max_cnts_src_pos = dmstat.out_max_loc 
                              
        src_x = max_cnts_src_pos.split(",")[0]
        src_y = max_cnts_src_pos.split(",")[1]
        
        if colname=="x":
            return src_x
        
        elif colname=="y":
            return src_y

        elif colname=="chipx":
            return int(float(src_x))
        
        elif colname=="chipy":
            return int(float(src_y))
    

##########################################################################
# Usage:
#  create_arf_ps( outtype, outroot, ebin, clobber, verbose, 
#              specfile, pbkfile, dafile, mskfile, ccd_id, skyx, skyy, chipx, chipy)
#
#
# Aim:
#   Run mkarf to create an unweighted ARF.  Run asphist and acis_fef_lookup
#   to create an aspect histogram and locate an FEF file, respectively, for
#   input to mkarf to create the ARF.  Return the name of the ARF.
#
#   
##########################################################################

#def create_arf_ps(full_outroot, evt_filename, infile, asp_param, ebin, clobber, verbose, specfile, pbkfile, dafile, mskfile, ccd_id, skyx, skyy, chipx, chipy):
def create_arf_ps(full_outroot, evt_filename, infile, asp_param, ebin, clobber, verbose, specfile, dafile, mskfile, ccd_id, skyx, skyy, chipx, chipy):

    # Use acis_fef_lookup to locate the appropriate FEF file for input
    # to mkrmf, which is the FEF-file-finding method to use for creating
    # unweighted RMFs.

    # If file does not have a CTI_APP header keyword, add one with
    # check_ctiapp.sh and issue a warning that the data should probably
    # be reprocessed.
    
    #if check_key(evt_filename, "CTI_APP")==0: 
    
    cti_app_val = get_keyval(evt_filename, "CTI_APP")

    if cti_app_val == "NONE":
        raise IOError("File {} is missing a CTI_APP header keyword, required by many CIAO tools; an ARF will not be created. Try re-running specextract after reprocessing your data.\n".format(evt_filename))
        #docommand("check_ctiapp.sh "+evt_filename)    

    acis_fef_lookup.punlearn()
     
    acis_fef_lookup.infile = evt_filename
    acis_fef_lookup.chipid = ccd_id #event_stats(infile, "ccd_id") 
    acis_fef_lookup.chipx  = chipx #event_stats(infile, "chipx")
    acis_fef_lookup.chipy  = chipy #event_stats(infile, "chipy")
    acis_fef_lookup.verbose = "0"

    acis_fef_lookup()

    feffile = acis_fef_lookup.outfile

    
    # Make mkarf 'detsubsys' keyword
    
    ccdid_mode_val = int(ccd_id) #event_stats(infile, "ccd_id")
     
    if ccdid_mode_val > 3:
        local_id = ccdid_mode_val - 4
        detname = "ACIS-S{}".format(local_id)
    else:
        local_id = ccdid_mode_val 
        detname = "ACIS-I{}".format(local_id)
    
    # ARF output file
    arffile = full_outroot + ".arf"

    mkarf.punlearn()
    
    mkarf.detsubsys     = detname
    mkarf.outfile       = arffile  
    mkarf.asphistfile   = asp_param
    mkarf.sourcepixelx  = skyx #event_stats(infile, "x") 
    mkarf.sourcepixely  = skyy #event_stats(infile, "y") 
    mkarf.grating       = get_keyval(evt_filename,"GRATING") 
    mkarf.obsfile       = evt_filename
    #mkarf.pbkfile       = pbkfile
    mkarf.dafile        = dafile 
    mkarf.maskfile      = mskfile      
    mkarf.verbose       = str(verbose)
    mkarf.engrid        = ebin
    mkarf.clobber       = clobber

    return mkarf(), arffile, feffile
        

########################################################
# Usage:
#   correct_arf(full_outroot, infile, evt_filename, orig_arf, skyx, skyy, binarfcorr)
#
#
# Aim:
#  Apply an energy-dependent point-source aperture correction
#  to the source ARF created by mkarf, if user has set the
#  'correct' specextract parameter to 'yes' and weight is 'no'.
#  It is not appropriate to run arfcorr on background ARFs because
#  background is extended.
#  
#  Return the name of the corrected ARF file.
#
########################################################

def correct_arf(full_outroot, infile, evt_filename, orig_arf, skyx, skyy, binarfcorr, clobber):

    gz_input = os.path.exists(evt_filename+".gz")
    guz_input = os.path.exists(evt_filename)

    if infile.endswith(".gz")==False and guz_input==False:

        if gz_input==True:

            evt_filename_gz = "{}.gz".format(evt_filename)

            infile = infile.replace(evt_filename, evt_filename_gz)

            v1("NOTE: {0} does not exist, but {0}.gz does; using the latter as input to arfcorr for the ARF correction.\n".format(evt_filename))
     
        else:
            infile = infile
               
    # arfcorr required input image:
    reg_image  = "{0}[bin sky={1}]".format(infile,binarfcorr)
             
    # --------------
    # ARF output file
    carffile = "{}.corr.arf".format(full_outroot)
    
    arfcorr.punlearn()
    arfcorr.infile  = reg_image  
    arfcorr.arf     = orig_arf
    arfcorr.outfile = carffile
    arfcorr.region  = get_region_filter(infile)[1]  
    arfcorr.x       = skyx
    arfcorr.y       = skyy
    arfcorr.energy  = 0
    arfcorr.verbose = 0
    arfcorr.clobber=  True

    return arfcorr(), carffile
    

    
##########################################################################
# Usage:
#   group_spectrum(ptype, full_outroot, val, spec, gtype,
#		    clobber, verbose, phafile)
#
# Aim:
#   Optionally group output spectrum.
#   
##########################################################################

def group_spectrum(ptype, full_outroot, val, spec, gtype,
                    clobber, verbose, phafile):

    # grouped spectrum name
    grpout = "{0}_grp.{1}".format(full_outroot, ptype.lower())

    dmgroup.punlearn()

    dmgroup.infile       =  "{}[SPECTRUM]".format(phafile)
    dmgroup.outfile      =  grpout 
    dmgroup.binspec      =  spec 
    dmgroup.grouptype    =  gtype 
    dmgroup.grouptypeval =  val 
    dmgroup.ycolumn      =  "counts" 
    dmgroup.xcolumn      =  "channel"
    dmgroup.tabcolumn    =  ""
    dmgroup.clobber      =  clobber
    dmgroup.verbose      =  verbose 

    return dmgroup(), grpout


###########################################################
# Usage:
#   edit_headers(verbose, infile, key, val)

# Aim:
#   Update/add the infile header key with a certain value.
#   
###########################################################

def edit_headers(verbose, infile, key, val, *args, **kwargs):

    unit = kwargs.get("unit",None) # optional argument
    comment = kwargs.get("comment",None)

    dmhedit.punlearn()

    dmhedit.infile    =  infile
    dmhedit.filelist  =  "none"
    dmhedit.operation =  "add"
    dmhedit.key       =  key 
    dmhedit.value     = str(val)
    dmhedit.verbose   = str(verbose) 

    if unit is not None:
        dmhedit.unit  = str(unit)

    if comment is not None:
        dmhedit.comment = str(comment)

    return dmhedit()


########################################################
# Usage:
#   get_filename( full_filename:s )
#
#
# Aim:
#  Pass in the full filename
#     i.e.  dir/source.fits[sky=region(dir/source.reg)]
#  and return the filename w/o filter
#     i.e.  filename = dir/source.fits
#

########################################################

def get_filename(full_filename):

    # filename = filename w/o filter

    #
    # NB: strip off the filter by scanning the string
    # until the first "[", "'[", or ""[".
    #
    
    brack = "["
    squot_brack = "'["
    dquot_brack = "\""+"["
    
    if dquot_brack in str(full_filename):
        filename = full_filename.split(dquot_brack)[0]

    elif squot_brack in str(full_filename):
        filename = full_filename.split(squot_brack)[0]

    else:
        filename = full_filename.split(brack)[0]
        
   

    if not filename or str(filename) in ["NULL", "NONE", "None",""," "]:     
        return NULL     

    else:
        return filename


 
########################################################
# Usage:
#   get_region_filter(full_filename)
#
#
# Aim:
#  Pass in the full filename
#     i.e.  dir/source.fits[sky=region(dir/source.reg)]
#  and return the filter without filename
#     i.e.  region_filter = region(source.reg)
#  for input to arfcorr
#

########################################################

def get_region_filter(full_filename):

    # region_filter = dm spatial region filter minus filename
    #                 and any other filters which may be present

    #
    # NB: strip off the filename by splitting the string
    # at "sky=" or "(x,y)=", and then cleaning up the dm
    # filter portion of this result.
    #
    # Note: the 'exclude' DM region filter syntax is unsupported
    # by sky2tdet for 'weight=yes'; by dmextract when 'weight=no';
    # and by dmmakereg when 'weight=no' and 'correct=yes'.
    # (dmextract won't output an error, but the WMAP doesn't make
    # it into the output spectrum as it should, causing a
    # calquiz error downstream).
    
    if "sky=" in full_filename:
        filter=1
        region_temp = full_filename.split("sky=")[1]

    elif "(x,y)=" in full_filename:
        filter=1
        region_temp = full_filename.split("(x,y)=")[1]

    elif "pos=" in full_filename:
        filter=1
        region_temp = full_filename.split("pos=")[1]

    else:
        filter=0
        
        if full_filename.startswith("@"):
            # deal with stacks
            pass
        else:        
            region_temp = full_filename
            v1("WARNING: A supported spatial region filter was not detected for {}\n".format(full_filename))        

    if filter==1:

        if ")," in region_temp and ") ," in region_temp:
            
            region_temp2 = region_temp.partition("),")[0]+")"

            if ") ," in region_temp2:
                region = region_temp2.partition(") ,")[0]+")"
            else:
                region = region_temp2
            
        elif ")," in region_temp:
            region = region_temp.partition("),")[0]+")"

        elif ") ," in region_temp:
            region = region_temp.partition(") ,")[0]+")"
           
        else:
            region = region_temp.rpartition("]")[0]
                     
    else:    
        region = full_filename

        
    if not region or str(region) in ["NULL", "NONE", "None",""," "]:
        raise IOError("Please specify a valid spatial region filter for {} or use FOV region files.".format(full_filename))

    else:
        return filter,region

########################################################
# Usage:
#   get_region(full_filename)
#
# NOT BEING USED AT THE MOMENT
#
# Aim:
#  Pass in the full filename
#     i.e.  dir/source.fits[sky=region(dir/source.reg)]
#  and return the region without filename or filter
#     i.e.  region = source.reg
#  for use in correct_arf() function
#

########################################################

def get_region(full_filename):

    # region = region without filename or filter syntax

    #
    # NB: strip off the filename by splitting the string
    # at "sky=", and then cleaning up the dm filter portion
    # of this result.
    #

    if "region" in get_region_filter(full_filename)[1]:
        region =  get_region_filter(full_filename)[1].split("(")[1].strip(")")
    else:
        region = get_region_filter(full_filename)[1]
        
    
    if not region or str(region) in ["NULL", "NONE", "None",""," "]:     
        return NULL     

    else:
        return region
    
################################################################
# Usage:
#  numCalFiles = call_calquiz(infile, product, queryStr,
#                              caldb_file, verbose)
#
# Aim:
#  Retrieve the caldb file name & number of files found
#  by calling calquiz.
#
################################################################

def call_calquiz(infile, product, query, file, verbose):


    #clear params and set calquiz command string
    calquiz.punlearn()

    calquiz.infile  =  infile + "[WMAP]"
    calquiz.product =  product 
    calquiz.calfile =  query  
    calquiz.outfile =  "y"
    calquiz.echo    = "yes"
    calquiz.verbose = verbose # NEW

    # run calquiz cmd, pget outfile if successful
    result = calquiz()   # can try calquiz.outfile here and perhaps
                         # remove "y" in outfile
    
    if result and "ERROR" not in str(result):
        
        cqpf = paramio.paramopen("calquiz", "r")

        
        if not cqpf:
            raise IOError("Unable to open calquiz parameter file.")

        else:

            outfiles = paramio.pgetstr(cqpf, "outfile") 

            outfiles_arr = numpy.array(outfiles.split(","))
          
            dims = outfiles_arr.shape   
            num_dims = outfiles_arr.ndim 
            data_type = outfiles_arr.dtype.name

            if 0 == len(outfiles_arr):
                numFiles = 0
                file = ""

            else: 
                # parse comma-separated outfile list to array
    
                file = outfiles_arr[0]
                numFiles = len(outfiles_arr)
                paramio.paramclose(cqpf)

            #domsg(verbose, 2, "CALDB " + str(product) + " file found was: "+str(file))

            # NB: This 'domsg' command is no longer necessary
            # b/c calquiz 'echo' parameter has been set to 'yes';
            # though screen output will be slightly different.
            
    else:
        numFiles=0
        file=None

    return [numFiles, file]

########################################################
# Usage:
#   determine_rmf_tool(infile, rmffile, verbose)
#
#
# Aim:
#  Decide whether to run mkrmf or mkacisrmf.
#  
#  Return the name of the tool to run.
#
########################################################

def determine_rmf_tool(infile,rmffile,verbose):

    caldb_p2_resp_file = ""   # P2_RESP CALDB file
    
    # NB: Adjust call_calquiz() function eventually
    #     to avoid this step of having to initialize
    #     the variable.
  
    # rmftool               # rmf tool to use: 'mkrmf' or 'mkacisrmf'
    # numCalFiles           # number of CALDB files found by calSearch
    
    # query the CALDB for the p2_resp file
    
    v2("Searching for P2_RESP calibration file...")

    calquiz_results = call_calquiz(infile,"SC_MATRIX",rmffile,caldb_p2_resp_file,verbose)
    numCalFiles = calquiz_results[0]
    
    caldb_p2_resp_file = calquiz_results[1]

    v3("Found the following P2_RESP file(s): {}".format(caldb_p2_resp_file))
    
    if not numCalFiles or numCalFiles in [0.0, 0, 0.]:
      
        v1("Cannot use mkacisrmf because no P2_RESP files were found.\n")

        v1("Please reprocess {} with acis_process_events if you wish to use mkacisrmf, unless the focal plane temperature is greater than -110C and the observation is taken in graded-mode or the extraction region is on a front-illuminated CCD.\n".format(infile))

        v1("Using mkrmf...\n")

        rmftool = "mkrmf"
  
    else:

        v1("Using mkacisrmf...\n")

        rmftool = "mkacisrmf"  

    return rmftool 


##########################################################################
# Usage:
#   doexit(message, outroot, srcbkg, ii, stack_outroot)
#
# Aim:
#   Print a warning when a tool fails. 
#   Remove the products for the current list entry.
#   Exit with a non-zero return value.
#   
##########################################################################

def doexit(message, outroot, srcbkg, ii, stack_outroot):

    v0(message + "\n")
    v0("Removing products related to current input file only.")

    cwd = os.getcwd()

    if "@" in str(outroot) or "," in str(outroot):
        if "/" in str(stack_outroot):
            v0("Please check {}/".format(os.path.dirname(stack_outroot)))
        else:
            v0("Please check {}/".format(cwd))

        v0("Removing file(s) {}.*".format(stack_outroot))
        cmd = "rm -f {}.*".format(stack_outroot)
        os.system(cmd)

    else:
        
        if "/" in str(outroot):
            v0("Please check {}/".format(os.path.dirname(outroot)))
        else:
            v0("Please check {}/".format(cwd))

        v0("Removing file(s) {0}_{1}{2}.*".format(outroot,srcbkg,ii))
        cmd = "rm -f {0}_{1}{2}.*".format(outroot,srcbkg,ii)
        os.system(cmd)

    raise SystemExit("Exiting.") #Is SystemExit the best choice, here?

##########################################################################
# Usage:
#  get_par(args)
#
# Aim:
#  Retrieve parameter values set in the referenced parameter file
#  and create a dictionary matching parameter name to parameter value.
##########################################################################

def get_par(args):
    """ Get specextract parameters from parameter file. """

    pinfo = open_param_file(args, toolname=toolname)
    pfile = pinfo["fp"]
    
#   Parameters:
    params={}
    pars={}
 
    pars["infile"] = params["infile"]     = paramio.pgetstr(pfile, "infile")
    pars["outroot"] = params["outroot"]    = paramio.pgetstr(pfile, "outroot")
    pars["weight"] = params["weight"]     = paramio.pgetstr(pfile, "weight")
    pars["weight_rmf"] = params["weight_rmf"]     = paramio.pgetstr(pfile, "weight_rmf")
    pars["correctpsf"] = params["correctpsf"]    = paramio.pgetstr(pfile, "correctpsf")
    pars["combine"] = params["combine"]    = paramio.pgetstr(pfile, "combine")
    pars["bkgfile"] = params["bkgfile"]    = paramio.pgetstr(pfile, "bkgfile")
    pars["bkgresp"] = params["bkgresp"]    = paramio.pgetstr(pfile, "bkgresp")  
    pars["asp"] = params["asp"]        = paramio.pgetstr(pfile, "asp")
    pars["refcoord"] = params["refcoord"] = paramio.pgetstr(pfile, "refcoord")
    pars["rmffile"] = params["rmffile"]    = paramio.pgetstr(pfile, "rmffile")
    #pars["ptype"] = params["ptype"]      = paramio.pgetstr(pfile, "ptype")
    pars["grouptype"] = params["gtype"]      = paramio.pgetstr(pfile, "grouptype")
    pars["binspec"] = params["gspec"]      = paramio.pgetstr(pfile, "binspec")
    pars["bkg_grouptype"] = params["bggtype"]    = paramio.pgetstr(pfile, "bkg_grouptype")
    pars["bkg_binspec"] = params["bggspec"]    = paramio.pgetstr(pfile, "bkg_binspec")
    pars["energy"] = params["ebin"]       = paramio.pgetstr(pfile, "energy")
    pars["channel"] = params["channel"]    = paramio.pgetstr(pfile, "channel")
    pars["energy_wmap"] = params["ewmap"]      = paramio.pgetstr(pfile, "energy_wmap")
    pars["binarfwmap"] = params["binarfwmap"]    = paramio.pgetstr(pfile, "binarfwmap")
    pars["binwmap"] = params["binwmap"]   = paramio.pgetstr(pfile, "binwmap")
    pars["binarfcorr"] = params["binarfcorr"]   = paramio.pgetstr(pfile, "binarfcorr")
    #pars["pbkfile"] = params["pbkfile"]    = paramio.pgetstr(pfile, "pbkfile")
    pars["dtffile"] = params["dtffile"]    = paramio.pgetstr(pfile, "dtffile")
    pars["mskfile"] = params["mskfile"]    = paramio.pgetstr(pfile, "mskfile")
    pars["dafile"] = params["dafile"]     = paramio.pgetstr(pfile, "dafile")
    pars["badpixfile"] = params["bpixfile"]   = paramio.pgetstr(pfile, "badpixfile")
    pars["tmpdir"] = params["tmpdir"] = paramio.pgetstr(pfile,"tmpdir")
    pars["clobber"] = params["clobber"]    = paramio.pgetstr(pfile, "clobber")
    pars["verbose"] = params["verbose"]    = paramio.pgeti(pfile, "verbose")
    pars["mode"] = params["mode"]       = paramio.pgetstr(pfile, "mode")

    #### Deprecated
    ##if params["ptype"].upper() != "PI":
    ##    raise ValueError("Support for ptype=PHA has been removed from specextract. Please contact the CXC HelpDesk at http://cxc.harvard.edu/helpdesk/ if you need to use this option.")

    # verify the existence of the output directory, create if non-existent
    if params["outroot"].startswith("@"): # although this approach still may run into problems with stacks
        stackfile = open(params["outroot"].lstrip("@"),"r")
        stackfile.close
        stackfile = stackfile.readlines()
        out_roots = [stackfile.replace(" ","").strip("\n") for stackfile in stackfile]
        del(stackfile)

        #out_roots = st.build(params["outroot"]) 
    else: 
        out_roots = params["outroot"].replace(" ","").split(",")

    for out_root in out_roots:
        if out_root.endswith("/"):
            raise ValueError("outroot path must include a file root name and cannot be just a directory.")

        outdir,outhead = utils.split_outroot(out_root)
        
        if outdir != "":
            fileio.validate_outdir(outdir)

    paramio.paramclose(pfile)
    return params,pars

##########################################################################
# Usage:
#  check_req_pars(pars=[])
#
# Aim:
#  Read the parameters which require user input to check if they
#  are null, exiting with an error if so.
#
##########################################################################

def check_req_pars(pars={}, req_pars=[]):
    """Reads the parameters which require user input to check if they are null,
    exiting with an error if so."""

    for i in req_pars:

        if not pars[i] or str(pars[i]) in ["NONE","none","None",""," ","0","0.0"]:

            par_error = 1
            
            if i=="asp":
                v0("The required parameter '{}' contains a null value. One or more aspect solution files, or one aspect histogram file, per input source event file is required in order to generate either weighted or unweighted source response files.".format(i))


            elif i=="mskfile":
                v0("The required parameter 'mskfile' contains a null value. One detector mask file per input source event file is required in order to generate either weighted or unweighted source ARF files.")


            elif i in ["weight","combine","correct"]:
                v0("The required parameter '{}' contains a null value; please enter 'yes' or 'no'.".format(i))


            elif i=="outroot":
                v0("The required parameter '{}' contains a null value. A root name for the output files must be supplied.".format(i))


            elif i=="infile":
                v0("The required parameter '{}' contains a null value. One or more source files must be supplied.".format(i))    

        else:
            par_error=0

    if par_error==1:
        raise ValueError("One or more required parameters is set to a null value. Exiting.")

        
##########################################################################
# 
# Main Code
#
##########################################################################

@handle_ciao_errors(toolname, __revision__)
def specextract(args):
 """ Run the tool """
 

 #-----------------------------------------------------------
 # Retrieve parameter values from specextract parameter file.
 #-----------------------------------------------------------

 params,pars = get_par(args)

 #--------------------------------------------------
 # Check the input values and do some initial setup.
 #--------------------------------------------------

##  # 1) Check that the parameters which require input are not null.

##  v1("\nChecking initial status and initializing variables...")

##  check_req_pars(params, ["infile","outroot","weight","combine","correct","asp","mskfile"])


 # 2) Set tool and module verbosity.

 set_verbosity(params["verbose"])
 utils.print_version(toolname, __revision__)
 v3("  Parameters: " + str(params))

 # 3) Define variables to represent parameter values.

 infile   =  params["infile"]    
 outroot  =  params["outroot"]
 weight   =  params["weight"]
 weight_rmf = params["weight_rmf"]
 correct  =  params["correctpsf"]
 combine  =  params["combine"]
 bkgfile  =  params["bkgfile"]
 bkgresp  =  params["bkgresp"]
 asp      =  params["asp"]
 refcoord =  params["refcoord"]
 rmffile  =  params["rmffile"]    
 ptype    =  "PI" #params["ptype"]   
 gtype    =  params["gtype"]  
 gspec    =  params["gspec"]      
 bggtype  =  params["bggtype"]    
 bggspec  =  params["bggspec"]   
 ebin     =  params["ebin"]       
 channel  =  params["channel"]    
 ewmap    =  params["ewmap"]      
 binwmap  =  params["binwmap"]    
 bintwmap =  params["binarfwmap"]
 binarfcorr = params["binarfcorr"]
 #pbkfile  =  params["pbkfile"]
 dtffile  =  params["dtffile"]
 mask     =  params["mskfile"]
 dafile   =  params["dafile"]
 bpixfile =  params["bpixfile"]
 tmpdir   =  params["tmpdir"]
 clobber  =  params["clobber"]   
 verbose  =  params["verbose"]    
 mode     =  params["mode"]

## error out if there are spaces in absolute paths of the various parameters
 if " " in os.path.abspath(infile):
         raise IOError("The absolute path for the infile, '{}', cannot contain any spaces".format(os.path.abspath(infile)))

 if " " in os.path.abspath(outroot):
         raise IOError("The absolute path for the outroot, '{}', cannot contain any spaces".format(os.path.abspath(outroot)))

 if " " in os.path.abspath(bkgfile):
         raise IOError("The absolute path for the bkgfile, '{}', cannot contain any spaces".format(os.path.abspath(bkgfile)))

 if " " in os.path.abspath(asp):
         raise IOError("The absolute path for the asp, '{}', cannot contain any spaces".format(os.path.abspath(asp)))

 if " " in os.path.abspath(dtffile):
         raise IOError("The absolute path for the dtffile, '{}', cannot contain any spaces".format(os.path.abspath(dtffile)))

 if " " in os.path.abspath(mask):
         raise IOError("The absolute path for the mskfile, '{}', cannot contain any spaces".format(os.path.abspath(mask)))

 if " " in os.path.abspath(rmffile):
         raise IOError("The absolute path for the rmffile, '{}', cannot contain any spaces".format(os.path.abspath(rmffile)))

 if " " in os.path.abspath(bpixfile):
         raise IOError("The absolute path for the badpixfile, '{}', cannot contain any spaces".format(os.path.abspath(bpixfile)))

 if " " in os.path.abspath(dafile):
         raise IOError("The absolute path for the dafile, '{}', cannot contain any spaces".format(os.path.abspath(dafile)))

##  try:
 # 4) Alert the user that the 'correct' parameter only applies
 #    when 'weight=no'.  Also check image/PSF binning factors for arfcorr and weighted ARFs

 if correct == "yes":
     if weight == "yes":
         v0("WARNING: The 'correct' parameter is ignored when 'weight=yes'.")
     else:
         v2("Note: all input source regions are converted to physical coordinates for point-source analysis with ARF correction.")

         # also check that the binarfcorr parameter is greater than zero
         if float(binarfcorr) <= 0:
             raise ValueError("'binarfcorr' must be greater than zero.")

 else:
     if float(bintwmap) <= 0 :
         raise ValueError("'binarfwmap' must be greater than zero.")

 # no need to duplicate responses, since src and bkg responses will be the same if refcoord!=""
 if refcoord != "" and bkgresp == "yes":
     v1("Responses for source and background are identical at {}, setting bkgresp=no to avoid duplicate files.".format(refcoord))
     bkgresp = "no"


 # 5) Define the binning specification for RMFs output by the script.

 if ptype == "PI":
     rmfbin = "pi={}".format(channel)
 else:
     rmfbin = "pha={}".format(channel)


 # 6) Build stacks for the file input parameters; make sure they are readable and not empty.

 # a) source stack

 # ~~~ NEW Nov. 20, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   
 if infile and infile not in [""," ","None","none","NONE"]:
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
     # handle a stack of regions, but single input file, in the format: "evt.fits[sky=@reg.lis]",
     # assume no other dmfilter included
     if get_region_filter(infile)[0] == 1 and get_region_filter(infile)[1].startswith("@") == True:
         regfile = get_region_filter(infile)[1]
         regfilter = fileio.get_filter(infile)
         fi = get_filename(infile)
         regcoord = regfilter.strip("\[\]").replace(regfile,"").replace("=","")

         regstk = st.build(regfile)
         
         src_stk = ["{0}[{1}={2}]".format(fi,regcoord,region) for region in regstk]

         del(fi)
         del(regfile)
         del(regfilter)
         del(regcoord)
         del(regstk)

     else:
         src_stk = st.build(infile) 

 # ~~~ NEW Nov. 20, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
 else:
     src_stk = [""]
     # script should fail in this case in the check_req_pars()
     # step above
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     

 src_count = len(src_stk)

 # error out if there are spaces in absolute paths of the stack
 for path in src_stk:
     if " " in os.path.abspath(path):
         raise IOError("The absolute path for the input file, '{}', cannot contain any spaces".format(os.path.abspath(path)))

 # check infiles for CC-mode data and throw warning; also look for merged data or blanksky files and error out
 for inf in src_stk:
     inf = fileio.get_file(inf)
     headerkeys = fileio.get_keys_from_file(inf)

     # check for blank sky files and maxim's bg files, error out if found:
     try:
         blanksky = headerkeys["CDES0001"]
     except:
         blanksky = None 

     try:
         mm_blanksky = headerkeys["MMNAME"]
     except:
         mm_blanksky = None

     if blanksky != None:
         if "blank sky event" in blanksky.lower():
             raise IOError("Cannot use blanksky background files as infile, since responses cannot be produced.\n")

     if mm_blanksky != None:
         if mm_blanksky.lower().startswith("acis") and "_bg_evt_" in mm_blanksky.lower():
             raise IOError("Cannot use M.M. ACIS blanksky background files as infile, since responses cannot be produced.\n")

     # check defined response energy range for weighted warm observations
     if weight == "yes":

        # check focal plane temperature, if >-110C, then check FEF energy range
        # ObsID 114 (observed 2000-01-30 10:40:42) is first observation made at <-119C.
        #
        # mkarf/mkrmf automatically creates response in the available energy range in the FEF file
        # for unweighted responses

        v3("Checking detector focal plane temperature\n")

        fp_temp = headerkeys["FP_TEMP"] - 273.15 # convert from Kelvin to Celsius

        if fp_temp > -110:
            v3("Checking FEF energy range for warm observation")

            acis_fef_lookup.punlearn()
            acis_fef_lookup.infile = inf
            acis_fef_lookup.chipid = "none"
            acis_fef_lookup.verbose = "0"
            acis_fef_lookup()

            fef = acis_fef_lookup.outfile

            cr_fef = pcr.read_file(fef)
            fef_energy = cr_fef.get_column("ENERGY").values
            cr_fef.__del__()

            fef_emin = min(fef_energy)
            fef_emax = max(fef_energy)

            ebin_min,ebin_max,ebin_de = ebin.split(":")
            
            fef_estat = (float(ebin_min) >= fef_emin, float(ebin_max) <= fef_emax)

            if fef_estat != (True,True):
                raise ValueError("ObsID {0} was made at a warm focal plane temperature, >-110C.  The available calibration products are valid for an energy range of {1:.3f}-{2:.3f} keV while the 'energy' parameter has been set to {3}-{4} keV (energy={5})".format(headerkeys["OBS_ID"],fef_emin,fef_emax,ebin_min,ebin_max,ebin)) # the ":.3f" in the format prints up to three decimal places of the float


     # find CC-mode data, throw warning
     if headerkeys["INSTRUME"] == "ACIS": 
         readmode = headerkeys["READMODE"]

         if readmode.upper() == "CONTINUOUS":
             v1("WARNING: Observation taken in CC-mode.  {0} may provide invalid responses for {1}.  Use rotboxes instead of circles/ellipses for extraction regions.".format(toolname,inf))
     
     # try to find if there is merged data in input stack, throw warning (or error out)
     merge_key = [headerkeys["TITLE"].lower(),
                  headerkeys["OBSERVER"].lower(),
                  headerkeys["OBJECT"].lower(),
                  headerkeys["OBS_ID"].lower()]

     try:
         merge_key.append(headerkeys["DS_IDENT"].lower())
     except KeyError:
         pass

     if "merged" in merge_key:
         raise IOError("Merged data sets are unsupported by {}.  Merged events files should not be used for spectral analysis.".format(toolname)) # or v1 warning?
     
     del(merge_key)

 # check infiles for ReproIV header keywords that came from the pbk and derived from the asol.
 check_file_pbkheader(src_stk)


 # b) outroot stack

 isoutstack = 1 
 outroot_tmp = outroot

 # ~~~ NEW Nov. 20, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   
 if outroot and outroot not in [""," ","None","none","NONE"]:
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     out_stk = st.build(outroot)

 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
 else:
     out_stk = [""]
     # script should fail in this case in the check_req_pars()
     # step above
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   

 out_count = len(out_stk)


 if 1 == out_count:

    # If the outroot stack count equals 1 but the source stack count
    # is not equal to 1, treat the outroot parameter as the only root
    # and append "src1", "src2", etc., to the output files.

    if src_count > 1:
        isoutstack = 0

 # error out if there are spaces in absolute paths
 for path in out_stk:
     if " " in os.path.abspath(path):
         raise IOError("The absolute path for the outroot, '{}', cannot contain any spaces".format(os.path.abspath(path)))


 # c) background stack; ensure it has the same
 #    number of elements as the source stack.

 if bkgfile and bkgfile not in [""," ","None","none","NONE"]:

     # handle a stack of regions, but single input file, in the format: "evt.fits[sky=@reg.lis]",
     # assume no other dmfilter included     
     if get_region_filter(bkgfile)[0] == 1 and get_region_filter(bkgfile)[1].startswith("@") == True:
         regfile = get_region_filter(bkgfile)[1]
         regfilter = fileio.get_filter(bkgfile)
         fi = get_filename(bkgfile)
         regcoord = regfilter.strip("\[\]").replace(regfile,"").replace("=","")

         regstk = st.build(regfile)
         
         bkg_stk = ["{0}[{1}={2}]".format(fi,regcoord,region) for region in regstk]

         del(fi)
         del(regfile)
         del(regfilter)
         del(regcoord)
         del(regstk)

     else:
         bkg_stk = st.build(bkgfile)

     bg_count = len(bkg_stk)

     # error out if there are spaces in absolute paths of the stacks
     for path in bkg_stk:
         if " " in os.path.abspath(path):
             raise IOError("The absolute path for the background file, '{}', cannot contain any spaces".format(os.path.abspath(path)))

     for bg in bkg_stk:
         if get_region_filter(bg)[0] == 0:
             raise IOError("Please specify a valid background spatial region filter for {} or use FOV region file.".format(bg))


     if src_count != bg_count:
         raise IOError("Source and background stacks must contain the same number of elements.  Source stack= " + str(src_count) + "    Background stack= " + str(bg_count))

     else:
         # Check that source and background ObsIDs match.
         src_obsid_stk = get_keyvals(src_stk, "OBS_ID")
         bkg_obsid_stk = get_keyvals(bkg_stk, "OBS_ID")

         # check for blank sky files and maxim's bg files:
         v1("Checking for blank sky background files...")
         
         with suppress_stdout_stderr():
             blanksky = get_keyvals(bkg_stk,"cdes0001")
             mm_blanksky = get_keyvals(bkg_stk,"mmname")
                      
         blanksky = [kw for kw in blanksky if kw != "-99" is not True]
         mm_blanksky = [kw for kw in mm_blanksky if kw != "-99" is not True]

         for blanksky_info in blanksky:
             if "blank sky event" in blanksky_info.lower():
                 if bkgresp == "yes":
                     raise IOError("Cannot create responses for spectra from blanksky background files.\n")
                 else:
                     v1("WARNING: Extracting background spectra from blanksky background files.\n")

         for mm_blanksky_info in blanksky:
             if mm_blanksky_info.lower().startswith("acis") and "_bg_evt_" in mm_blanksky_info.lower():
                 if bkgresp == "yes":
                     raise IOError("Cannot create responses for spectra from M.M. ACIS blanksky background files.\n")
                 else:
                     v1("WARNING: Extracting background spectra from M.M. ACIS blanksky background files.\n")

         if "-99" not in src_obsid_stk and "-99" not in bkg_obsid_stk:

             src_obsid_stk_int=[int(i) for i in src_obsid_stk]
             bkg_obsid_stk_int=[int(i) for i in bkg_obsid_stk]

             # Check that src&bkg stacks have matching ObsID values and also same number of each unique value. 

             if sum(abs(numpy.array(sorted(src_obsid_stk_int))-numpy.array(sorted(bkg_obsid_stk_int)))) != 0:
                 v0("WARNING: Background file OBS_IDs differ from source file OBS_IDs; ignoring background input.\n")
                 dobg = 0
                 dobkgresp = 0
                 fcount = src_count

             # Check if the matched src&bkg ObsID values are entered in the proper matching order.

             elif sum(abs(numpy.array(src_obsid_stk_int)-numpy.array(bkg_obsid_stk_int))) != 0:
                 v0("WARNING: 'bkgfile' file order does not match 'infile' file order; ignoring background input.\n")
                 dobg = 0
                 dobkgresp = 0
                 fcount = src_count

             else:
                 dobg = 1
                 fcount = src_count

                 if bkgresp == "yes":
                     dobkgresp = 1
                 else:
                     dobkgresp = 0

         else:
             v0("WARNING: OBS_ID information could not be found in one or more source and/or background files. Assuming source and background file lists have a matching order.\n")
             dobg = 1
             fcount = src_count

             if bkgresp == "yes":
                 dobkgresp = 1
             else:
                 dobkgresp = 0

 else: 
     dobg = 0
     dobkgresp = 0
     fcount = src_count

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 # check counts in input files, and exit if necessary, or produce responses for upper-limits
 for src in src_stk:
     check_event_stats(src,refcoord,False)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 # find ancillary files, if exists, add to stack
 ancil = {}
 ancil["asol"] = []
 ancil["bpix"] = []
# ancil["pbk"] = []
 ancil["msk"] = []
 ancil["dtf"] = []

 for obs in src_stk:
     fobs = obsinfo.ObsInfo(obs)

     #if "" in [asp,bpixfile,pbkfile,mask]:
     if "" in [asp,bpixfile,mask]:
         v1("Using event file {}\n".format(obs))

     if asp == "":
         v3('Looking in header for ASOLFILE keyword\n')
         asols = fobs.get_asol()
         asolstr = ",".join(asols)
         # It should be okay to have multiple entries per source since
         # downstream code tries to match observations to asol files,
         # but is this true or the best way to do it?
         ancil["asol"].extend(asols)
         if len(asols) == 1:
             suffix = ''
         else:
             suffix = 's'
         v1("Aspect solution file{} {} found.\n".format(suffix, asolstr))

     if bpixfile == "":
         v3('Looking in header for BPIXFILE keyword\n')         
         ancil["bpix"].append(fobs.get_ancillary("bpix"))
         v1("Bad-pixel file {} found.\n".format(fobs.get_ancillary("bpix")))

     if mask == "":
         v3('Looking in header for MASKFILE keyword\n')
         ancil["msk"].append(fobs.get_ancillary("mask"))
         v1("Mask file {} found.\n".format(fobs.get_ancillary("mask")))

     if fobs.instrument == "ACIS":
         ancil["dtf"].append("")
               
     elif dtffile == "":
         v3('Looking in header for DTFFILE keyword\n')
         ancil["dtf"].append(fobs.get_ancillary("dtf"))
         v1("HRC dead time factor file {} found.\n".format(fobs.get_ancillary("dtf")))
             

 # cast as stacks
 if asp == "" and len(ancil["asol"]) != 0:
         temp_asol_stk = make_stackfile(ancil["asol"],suffix="asol",delete=True)

 if bpixfile == "" and len(ancil["bpix"]) != 0:
         temp_bpix_stk =  make_stackfile(ancil["bpix"],suffix="bpix",delete=True)

 if dtffile == "" and len(ancil["dtf"]) != 0:
         if "" not in ancil["dtf"]:
             temp_dtf_stk = make_stackfile(ancil["dtf"],suffix="dtf",delete=True)
         
 if mask == "" and len(ancil["msk"]) != 0:
         temp_msk_stk = make_stackfile(ancil["msk"],suffix="msk",delete=True)

 # assign stacks to variables
 if False not in [asp.lower() != "none", asp == ""]:
     asp = "@"+temp_asol_stk.name

 if False not in [dtffile.lower() != "none", dtffile == ""] and "" not in ancil["dtf"]:
     dtffile = "@"+temp_dtf_stk.name

 if False not in [bpixfile.lower() != "none", bpixfile == ""]:
     bpixfile = "@"+temp_bpix_stk.name

 if False not in [mask.lower() != "none", mask == ""]:
     mask = "@"+temp_msk_stk.name

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


 #    d)  pbkfile stack; ensure it has
 #        either 1 or src_count elements

 # ~~~ NEW Nov. 7, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 #if pbkfile and pbkfile not in [""," ","None","none","NONE"]:    
 ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 #    pbk_stk = st.build(pbkfile)
 #
 #    check_files(pbk_stk,"parameter block")
 #    pbk_count = len(pbk_stk)
 #
 #    if 1 != pbk_count and src_count != pbk_count:
 #
 #        raise IOError("Error: pbkfile stack must have either 1 element or the same number of elements as the source stack.  Source stack= " + str(src_count) + "    pbkfile stack= " + str(pbk_count))
 #
 ## ~~~ NEW Nov. 7, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 #else:
 #    pbk_stk=[""]
 #    pbk_count = len(pbk_stk)
 ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


 #    e) dafile stack; ensure it has
 #       either 1 or src_count elements


 # ~~~ NEW Nov. 7, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 if dafile and dafile not in [""," ","None","none","NONE"]:
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    

     da_stk = st.build(dafile)
     check_files(da_stk, "dead area")
     da_count = len(da_stk)

     if 1 != da_count and src_count != da_count:

         raise IOError("Error: dafile stack must have either 1 element or the same number of elements as the source stack.  Source stack={0}     dafile stack={1}".format(src_count,da_count))
 # ~~~ NEW Nov. 7, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 else:
     da_stk = [""]
     da_count = len(da_stk)
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


 #    f) mskfile stack; ensure it has
 #       either 1 or src_count elements

 # ~~~ NEW Nov. 20, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 if mask and mask not in [""," ","None","none","NONE"]:
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     mask_stk = st.build(mask) 
     check_files(mask_stk, "mask")
     mask_count = len(mask_stk)

     if 1 != mask_count and src_count != mask_count:

         raise IOError("The 'mskfile' stack must have either 1 element or the same number of elements as the source stack.  Source stack={0}     mskfile stack={1}".format(src_count,mask_count))
 # ~~~ NEW Nov. 20, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
 else:
     mask_stk = [""]
     mask_count = len(mask_stk)
     # script should fail in this case in the check_req_pars()
     # step above
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


 #    g) badpixfile file stack; ensure it has
 #       either 1 or src_count elements

 if bpixfile and bpixfile not in [""," ","None","none","NONE"]:     

     bpix_stk = st.build(bpixfile)
     check_files(bpix_stk,"bad pixel")
     bpix_count = len(bpix_stk)

     if 1 != bpix_count and src_count != bpix_count:
         raise IOError("The 'badpixfile' stack must have either 1 element or the same number of elements as the source stack.  Source stack={0}     bpix stack={1}".format(src_count,bpix_count))
     else:
         dobpix=1
 else:
     bpix_stk = [""]
     dobpix=0


 #    h) aspect histogram/solution file stack
 #
 #       If histogram, it must have 1 or
 #        src_count elements.
 #
 #       If solution, it can have
 #       1 or more elements per source file.

 # ~~~ NEW Nov. 20, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 if asp and asp not in [""," ","None","none","NONE"]:
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

     asp_stk = st.build(asp)
     check_files(asp_stk,"aspect")
     asp_count = len(asp_stk)


 #    i)  dtffile stack; ensure it has
 #        either 1 or src_count elements

 # ~~~ NEW Sep. 27, 2013 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 if dtffile and dtffile not in [""," ","None","none","NONE"]:    
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     dtf_stk = st.build(dtffile)

     check_files(dtf_stk,"dead time factor")
     dtf_count = len(dtf_stk)

     if 1 != dtf_count and src_count != dtf_count:

         raise IOError("Error: dtffile stack must have either 1 element or the same number of elements as the source stack.  Source stack={0}     dtffile stack={1}".format(src_count,dtf_count))
 # ~~~ NEW Sep. 27, 2013 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 else:
     dtf_stk=[""]
     dtf_count = len(dtf_stk)
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


 #
 # check that there are no spaces in ancillary file paths, if specified
 #
 for path in asp_stk:
     if " " in os.path.abspath(path):
         raise IOError("The absolute path for the asol file, '{}', cannot contain any spaces".format(os.path.abspath(path)))

 for path in bpix_stk:
     if " " in os.path.abspath(path):
         raise IOError("The absolute path for the badpix file, '{}', cannot contain any spaces".format(os.path.abspath(path)))

 if bpix_stk == [""]:
     del(bpix_stk)

 for path in mask_stk:
     if " " in os.path.abspath(path):
         raise IOError("The absolute path for the mask file, '{}', cannot contain any spaces".format(os.path.abspath(path)))

 for path in dtf_stk:
     if " " in os.path.abspath(path):
         raise IOError("The absolute path for the DTF file, '{}', cannot contain any spaces".format(os.path.abspath(path)))

 for path in da_stk:
     if " " in os.path.abspath(path):
         raise IOError("The absolute path for the dead area file, '{}', cannot contain any spaces".format(os.path.abspath(path)))


 #    Determine if asphist or asol files were input
 #    to the 'asp' parameter and build the appropriate
 #    stack accordingly. (Must check all files since
 #    the invoked tool 'asphist' won't complain if
 #    histogram files are incorrectly input.)

 aspfile_hdus = get_keyvals(asp_stk, "HDUCLAS2") 


 if "HISTOGRAM" in aspfile_hdus and "ASPSOL" in aspfile_hdus:
     raise IOError("A mix of aspect solution and histogram files were entered into 'asp'; please enter one or a list of either type, not both.\n")

 elif "ASPSOL" in aspfile_hdus:
     for i in aspfile_hdus:
         if i != "ASPSOL":
             raise IOError("Found a file in 'asp' which is neither an aspect solution nor histogram file. Exiting.\n")
         else:
             asol=1
             ahist=0

 elif "HISTOGRAM" in aspfile_hdus:
     for i in aspfile_hdus:
         if i != "HISTOGRAM":
             raise IOError("Found a file in 'asp' which is neither an aspect solution nor histogram file. Exiting.\n")
         else:
             asol=0
             ahist=1

 else:
     raise IOError("Neither aspect histogram nor aspect solution files were found in the 'asp' input. Either the ASPHIST/asphist or ASPSOL FITS HDU is not in the expected place - which could cause the CIAO tools invoked by this script to fail - or a filename was entered incorrectly or does not exist. Exiting.\n")   


 if ahist:
     if 1 != asp_count and src_count != asp_count:
         raise IOError("Error: asp stack must have either 1 element or the same number of elements as the source stack.  Source stack={0}    asp stack={1}".formt(src_count,asp_count))

 elif asol:

     # Build the aspect solution ('asol') file stack:

     asol_stk = asp_stk
     asol_count = asp_count

     if 1 != asol_count and 1 != src_count:

     # Compile lists of the OBS_IDs found in the
     # input source and asol file headers, exiting with
     # an error if the requisite OBS_ID information
     # is not found or is mismatched.

         src_obsid_stk = get_keyvals(src_stk, "OBS_ID")    
         asol_obsid_stk = get_keyvals(asol_stk, "OBS_ID") 


         #if dobg==1:

         #    bkg_obsid_stk = get_keyvals(bkg_stk, "OBS_ID")

         #    if "-99" in bkg_obsid_stk:
         #        v0("OBS_ID information could not be found in one or more background files; cannot properly match background files to aspect solution files.")

         #        dobg==0

         if "-99" in src_obsid_stk or "-99" in asol_obsid_stk:
             raise ValueError("OBS_ID information could not be found in source and/or aspect solution files; cannot properly match source files to aspect solution files. Try entering aspect histogram files into 'asp' to work around this requirement.")


         # Quit with an error if any of the source file OBS_ID
         # values are not found in the list of asol OBS_IDs.

         for i in range(0, src_count):
             if str(src_obsid_stk[i]) not in asol_obsid_stk:
                 raise IOError("No aspect solution files provided for {}.".format(src_stk[i]))

             #if dobg==1:
             #    if str(bkg_obsid_stk[i]) not in asol_obsid_stk:
             #        raise IOError("No aspect solution files provided for %s." % (stk_read_num(bkg_stk, i+1)))



         asol_sorted_grouped = group_by_obsid(asol_stk,"aspsol")

         v3("ObsIDs matched to aspect solution files: \n{}".format(asol_sorted_grouped))



     elif 1 != asol_count and 1 == src_count:

         asol_sorted = sort_files(asol_stk, "aspsol", "TSTART")


 # ~~~ NEW Nov. 20, 2012 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    
 else:
     asp_stk=[""]
     asp_count = len(asp_stk)
     # script should fail in this case in the check_req_pars()
     # step above
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 #----------------------------------------------------------
 # Determine whether or not to group source output spectrum,
 # and set appropriate grouping values.
 #
 # dogroup           # flag set if grouping
 # binspec           # grouping spec
 # gval              # grouping value
 #-----------------------------------------------------------

 if "NONE" == gtype: 
     dogroup = 0

 else:
     dogroup = 1

     if "BIN" == gtype: 
         binspec = gspec
         gval = ""

     else:
         binspec = ""
         gval = gspec


 #----------------------------------------------------------
 # Determine whether or not to group background output spectrum
 # and set appropriate grouping values.
 #
 # bgdogroup           # flag set if grouping
 # bgbinspec           # grouping spec
 # bggval              # grouping value
 #----------------------------------------------------------

 if "NONE" == bggtype:
     bgdogroup = 0

 else:
     bgdogroup = 1

     if "BIN" == bggtype:
         bgbinspec = bggspec
         bggval = ""

     else:
         bgbinspec = ""
         bggval = bggspec


 #--------------------------------------------------------------------
 # Determine whether or not to combine source output spectra, and any
 # associated background spectra and response files, if user has input
 # a stack of source event files. 
 #
 # docombine         # flag set if combining
 #----------------------------------------------------------

 if combine=="yes":

     if src_count < 2:
         print("Warning: There are fewer than two source event files specified in the 'infile' parameter; the 'combine=yes' setting will be ignored.\n")

         docombine = 0

         cs_src_spectra = [0]
         cs_src_arfs = [0]
         cs_src_rmfs = [0]
         cs_bkg_spectra = [0]
         cs_bkg_arfs = [0]
         cs_bkg_rmfs = [0]


     else:    
         docombine = 1

         cs_src_spectra = list(range(src_count))
         cs_src_arfs = list(range(src_count))
         cs_src_rmfs = list(range(src_count))

         sphafiles = list(range(src_count))
         sarffiles = list(range(src_count))
         srmffiles = list(range(src_count))

         if dobg==1:

             cs_bkg_spectra = list(range(src_count))
             cs_bkg_arfs = list(range(src_count))
             cs_bkg_rmfs = list(range(src_count))

             bphafiles = list(range(src_count))
             barffiles = list(range(src_count))
             brmffiles = list(range(src_count))

         else:

             cs_bkg_spectra = [0]
             cs_bkg_arfs = [0]
             cs_bkg_rmfs = [0]
 else:

     docombine = 0

     cs_src_spectra = [0]
     cs_src_arfs = [0]
     cs_src_rmfs = [0]
     cs_bkg_spectra = [0]
     cs_bkg_arfs = [0]
     cs_bkg_rmfs = [0]


 # ///////////////////////////////////////////////////////////////////////////
 #
 # Reference for the key variables used in the main "for" loop of the script,
 # below (carried over from the original specextract and expanded to include
 # some new variables). Some of the variables in the list are already defined 
 # at this point, and some are defined below. 

 # ii = 0                # counter
 # fullfile              # infile string
 # filename              # infile string w/o filter
 # pbk_arg               # pbkfile arg passed to mkwarf
 # da_arg                # dafile arg passed to mkwarf
 # msk_arg               # mskfile arg passed to mkwarf
 #
 # ret                   # function return value
 # ancrfile              # ARF filename returned from create_arf
 # weightfile            # weight file from create_arf
 # phafile               # extracted spectrum filename 
 # grpfile               # grouped spectrum filename
 # rmftool               # rmf tool to use: 'mkrmf' or 'mkacisrmf'
 # respfile              # RMF filename returned from build_rmf
 # otype                 # which filetypes to process ("src", "bkg")
 # cur_stack             # current stack - bkg or src
 # asol_arg              # current aspect solution file or list of files
 # asp_arg               # source aspect histogram file input by user or
 #                       # output by mk_asphist
 #
 #//////////////////////////////////////////////////////////////////////////


 #----------------------------------------------------------
 # Determine the file output types, either source files or
 # both source and background files.
 #----------------------------------------------------------

 if 1 == dobg:
   otype = ["src","bkg"]
 else:
   otype = ["src"]

 #------------------------------------------------------------------
 # Check all of the input files up front and make sure they are
 # readable. If not, notify the user of each bad file and exit.
 #
 # srcbkg                # the current output type
 # inputfile             # current input file to test
 # table                 # is the infile readable (!NULL)?
 # badfile               # is at least one of the input files bad?
 #
 # Do for each output type we are processing,
 # i.e. "src" or ( "src" and "bkg" )
 #
 #------------------------------------------------------------------- 

 for i in otype:

     # Set srcbkg to the current member of otype.
     srcbkg = i

     #
     # Determine which stack to use.
     #
     if "bkg" == srcbkg:

         #v1("\nChecking background input file(s) for readability...")
         cur_stack = bkg_stk

     else:

         #v1("\nChecking source input file(s) for readability..." )
         cur_stack = src_stk


     #
     # Look at each file in the stack and check for readability.
     #

     if "bkg" == srcbkg:
         check_files(cur_stack,"background")
     else:
         check_files(cur_stack,"source")



 #---------------------------------------------------------
 # For each stack item in the source and background lists:
 #
 #    ) optionally set the ardlib bad pixel file
 #    ) convert src regions to phys. coords for ARF correction
 #    ) extract the spectrum
 #    ) create an ARF 
 #    ) create a RMF
 #    ) optionally group the spectrum
 #    ) add header keywords
 #    ) optionally combine output spectra and responses
 #
 #----------------------------------------------------------

 # Do for each output type we are processing,
 #  i.e. "src" or ( "src" and "bkg" )

 for i in otype:

     # Set srcbkg to the current member of otype.
     srcbkg = i

     #
     # Determine which stack to use.
     #
     if "bkg" == srcbkg:
         cur_stack = bkg_stk
     else:
         cur_stack = src_stk

     # outroot + src/bkg + output number; ie. 'outroot_src1'
     # full_outroot

     #
     # Run tools for each item in the current stack.
     #
     #for ii in range(1, fcount+1):
     for ii in range(fcount):
         fullfile = str(cur_stack[ii]) # str(stk_read_num(cur_stack, ii))
         filename = get_filename(cur_stack[ii])

         if fcount == 1:
             iteminfostr = "\n"
         else:
             iteminfostr = "[{0} of {1}]\n".format(str(ii+1),str(fcount))

         instrument = get_keyval(filename,"instrume")

         if instrument == "HRC" and weight == "yes":
             weight = "no"
             v1("HRC responses will be unweighted.")

         if check_event_stats(fullfile,refcoord,True) == False:
             weight = "no"

         #  If we're using an output stack, then grab an item off of the stack
         #  but if not, then append "src1", "src2", etc., to the outroot
         #  parameter for each output file.

         if 1 == isoutstack:
             outdir, outhead = utils.split_outroot(out_stk[ii])

             if outhead == "":
                 full_outroot = outdir + srcbkg #stk_read_num(out_stk, ii) + "_bkg"

             else:
                 if "bkg" == srcbkg:
                     full_outroot = outdir + outhead + srcbkg
                 else:
                     full_outroot = outdir + outhead.rstrip("_")
                     
         else:
             outdir, outhead = utils.split_outroot(outroot)

             if outhead == "":
                 full_outroot = outdir + srcbkg + str(ii+1)
             else:
                 full_outroot = outdir + outhead + srcbkg + str(ii+1)

         #
         # If pbk_count or da_count are > 1, read an item from
         # each stack;
         # only need to check one, since at this point pbk_count == da_count.
         #

         ## Set pbkfile argument passes to create_arf_*.
         #if 1 != pbk_count:
         #    pbk_arg = pbk_stk[ii] #stk_read_num(pbk_stk, ii)
         #else:
         #    if pbkfile.startswith("@"):
         #        pbk_arg = ",".join(st.build(pbkfile))
         #    else:
         #        pbk_arg = pbkfile


         # Set dafile argument passes to create_arf_*.
         if 1 != da_count:
             da_arg  = da_stk[ii] #stk_read_num(da_stk, ii)
         else:
             if dafile.startswith("@"):
                 da_arg = ",".join(st.build(dafile))
             else:
                 da_arg  = dafile


         # Set dtffile argument passes to mk_asphist().
         if 1 != dtf_count:
             dtf_arg = dtf_stk[ii] #stk_read_num(dtf_stk, ii)
         else:
             if dtffile.startswith("@"):
                 dtf_arg = ",".join(st.build(dtffile))
             else:
                 dtf_arg = dtffile


         # If asol and not asphist files were input to
         # 'asp' parameter, set asol argument passes to
         # mk_asphist() for creating aspect histogram file
         # input to create_arf_*.
         if asol:

            if 1 != asol_count and 1 != src_count:

                # Moved the following two commented-out lines of code higher up
                # in script, outside of the main loop, to avoid the
                # segmentation faulting which occurs when group_by_obsid()
                # accesses a certain constant in the cxcdm module 
                # (via get_block_info_from_file) too many times.

                #asol_sorted_grouped = group_by_obsid(asol_stk,"aspsol")
                #v1("\nObsIDs matched to aspect solution files: \n"+str(asol_sorted_grouped))

                asol_arg = asol_sorted_grouped[src_obsid_stk[ii]] #was '[ii-1]'

                # yes, use src_obsid_stk for both src and bkg 
                # until we support src & bkg input from
                # separate observations (though to get to this
                # point, src obs should equal bkg obs anyway)

            elif 1 != asol_count and 1 == src_count:

                asol_arg = asol_sorted

            else:
                asol_arg = asp

         elif ahist:
             # set asol_arg for resp_pos
             asol_arg = "none"

             if 1 != asp_count:
                 aspfile_block = get_block_info_from_file(asp_stk[ii])

             else:
                 aspfile_block = get_block_info_from_file(asp)


             if aspfile_block[0].find("asphist") != -1:
                 if srcbkg != "bkg":
                     v1("Found a Level=3 ahst3.fits file in 'asp' input; the 'asphist' block corresponding to the source region location will be used.\n")

                 if 1 != asp_count:
                     asp_arg = asp_stk[ii]+"[asphist"+str(event_stats(fullfile, "ccd_id"))+"]"
                 else:
                     if asp.startswith("@"):
                         asp_arg = ",".join(st.build(asp))+"[asphist"+str(event_stats(fullfile, "ccd_id"))+"]"
                     else:
                         asp_arg = asp+"[asphist"+str(event_stats(fullfile, "ccd_id"))+"]"

             else:
                 if 1 != asp_count:
                     asp_arg  = asp_stk[ii] #stk_read_num(asp_stk, ii)        
                 else:
                     if asp.startswith("@"):
                         asp_arg = ",".join(st.build(asp))
                     else:
                         asp_arg  = asp


         # Set mask argument passes to create_arf_ext (mkwarf).

         if 1 != mask_count:
             msk_arg  = mask_stk[ii] #stk_read_num(mask_stk, ii)
         else:
             if mask.startswith("@"):
                 msk_arg = ",".join(st.build(mask))
             else:
                 msk_arg  = mask

         # Set bpixfile argument passes to set_badpix().

         if dobpix==1:

             if 1 != bpix_count:
                 bpix_arg  = bpix_stk[ii] #stk_read_num(bpix_stk, ii)
             else:
                 if bpixfile.startswith("@"):
                     bpix_arg = ",".join(st.build(bpixfile))
                 else:
                     bpix_arg  = bpixfile

         # determine coordinates to use to produce responses
         ra,dec,skyx,skyy,chipx,chipy,chip_id = resp_pos(fullfile,asol_arg,refcoord)

         # determine hydrogen column density based on extraction region centroid 
         # or refcoord to add to PI header in units of 1e-22 cm**-2 
         nrao_nh = colden(ra,dec,dataset="nrao")
         if nrao_nh not in [None,"-",0.0]:
             nrao_nh *= 0.01

         bell_nh = colden(ra,dec,dataset="bell") 
         if bell_nh not in [None,"-",0.0]:
             bell_nh *= 0.01
         else:
             v1("Warning: Skip adding 'bell_nh' header keyword.  No valid data at the source location in the Bell Labs HI Survey (the survey covers RA > -40 deg).\n") 
             
         del(ra,dec)

         if instrument == "ACIS":
             if int(chipx) < 1:
                 v1("chipx={} for ACIS-{}; using chipx=1 for FEF and RMF look up.\n".format(chipx,chip_id))
                 chipx = "1"
             if int(chipy) < 1:
                 v1("chipy={} for ACIS-{}; using chipy=1 for FEF and RMF look up.\n".format(chipy,chip_id))
                 chipy = "1"
             if int(chipx) > 1024:
                 v1("chipx={} for ACIS-{}; using chipx=1024 for FEF and RMF look up.\n".format(chipx,chip_id))
                 chipx = "1024"
             if int(chipy) > 1024:
                 v1("chipy={} for ACIS-{}; using chipy=1024 for FEF and RMF look up.\n".format(chipy,chip_id))
                 chipy = "1024"

         # Set the rmffile argument pass to determine_rmf_tool() [for ACIS only].
         if instrument == "ACIS":
             ccdid_val = "(ccd_id={})".format(chip_id)

             if rmffile=="CALDB":
                 rmffile_ccd = rmffile+ccdid_val
                 null_rmffile=False

             elif "CALDB(" in str(rmffile):
                 rmffile_ccd = rmffile
                 null_rmffile=False

             else:
                 rmffile_ccd = "CALDB"+ccdid_val
                 null_rmffile = True


         ###################################
         #
         # set bad pixel file in ardlib.par
         #
         ##################################
         if dobpix==1:

             v1("Setting bad pixel file {}".format(iteminfostr))
             ret = set_badpix(filename, bpix_arg, instrument, verbose)

             # NB: If 'ret' is null (b/c acis_set_ardlib fails), the script should
             # terminate here and exit with a acis_set_ardlib error, even if verbose=0.

             if not ret and verbose != 0:
                 raise IOError("Failed to set {0} bad pixel file in ardlib.par for {1}.".format(bpix_arg,filename))


         #####################################
         #    
         # convert source region to physical
         # coordinates for ARF correction
         #
         #####################################

         # First, reset the 'correct' parameter to its original
         # (user input) value in case it was changed from
         # 'yes' to 'no' in the last pass through the loop
         # (e.g., when an unsupported source region is entered
         # for src file 1 and a supported one for src file 2)

         if weight != "yes":

             correct = params["correctpsf"]

             if correct == "yes" and srcbkg == "src":

                 if get_region_filter(fullfile)[0] != 1:
                     v0("WARNING: The ARF generated for {} cannot be corrected as no supported spatial region filter was detected for this file, which is required input for this step.\n".format(fullfile))

                     correct = "no"
                     fullfile = fullfile

                 else:

                     v1("Converting source region to physical coordinates {}".format(iteminfostr))
                     #outreg = full_outroot+"_phys_coords_"+srcbkg+str(ii+1)+".reg"
                     #fullfile = convert_region(fullfile, filename, outreg, clobber, verbose)

                     outreg = tempfile.NamedTemporaryFile(suffix="_phys_coords_{0}{1}.reg".format(srcbkg,str(ii+1)),dir=tmpdir) 
                     fullfile = convert_region(fullfile, filename, outreg.name, "yes", verbose)
                     
             else:
                 fullfile = fullfile


         ###########################
         #
         # extract spectrum
         #
         ###########################

         v1("Extracting {0} spectra {1}".format(srcbkg,iteminfostr))

         # If 'binwmap' contains 'tdet' string, check that TDET column exists
         # in file; if not, change 'binwmap' value to
         # 'det={user's specification}' and notify user.

         if "tdet" in binwmap:

##              cr = pcr.read_file(fullfile)
##              if cr == None:
##                  raise IOError("Unable to read from file %s" % fullfile)

##              #if pcr.get_col(cr, "tdet") != 1:
##              if "tdet" not in str(pcr.get_col_names(cr)):

##                  v1("WARNING: No TDET column found in %s; the 'wmap' parameter of dmextract will be set to use DET coordinates instead.\n" % (filename))

##                  binwmap_val = binwmap.split("=")[1]
##                  binwmap = "det="+binwmap_val

##              else:
##                  binwmap=binwmap


             ######
             # work around pycrates issue when operating on a crates 
             # with zero rows by using cxcdm instead of crates
             ######
             try:
                 bl = cxcdm.dmBlockOpen(fullfile)
             except IOError:
                 raise IOError("Unable to read from file {}".format(fullfile))

             fullfile_columns = [cxcdm.dmGetName(dd).lower() for dd in cxcdm.dmTableOpenColumnList(bl)]
             
             if "tdet" not in fullfile_columns:
                 v1("WARNING: No TDET column found in {}; the 'wmap' parameter of dmextract will be set to use DET coordinates instead.\n".format(filename))

                 binwmap_val = binwmap.split("=")[1]
                 binwmap = "det="+binwmap_val

             else:
                 binwmap = binwmap

             # clean up opening file
             cxcdm.dmBlockClose(bl)
             del(fullfile_columns)
                 
         (ret, phafile) = extract_spectra(full_outroot, fullfile, ptype, 
                                          ewmap, binwmap, instrument, clobber, verbose)
         
         # add nH values to phafile header
         if nrao_nh not in [None,"-",0.0]:
             edit_headers(verbose, phafile, "NRAO_nH", nrao_nh, unit="10**22 cm**-2", comment="galactic HI column density") #comment="galactic neutral hydrogen column density at the source position using the NRAO all-sky interpolation, via COLDEN."
  
         if bell_nh not in [None,"-",0.0]:
             edit_headers(verbose, phafile, "Bell_nH", bell_nh, unit="10**22 cm**-2", comment="galactic HI column density") #comment="galactic neutral hydrogen column density at the source position using the Bell Labs survey, via COLDEN."         
         

         # NB: If 'ret' is null (b/c dmextract() fails), the script should
         # terminate here and exit with a dmextract error, even if verbose=0.

         if not ret and verbose != 0:
             doexit("Failed to extract spectrum for " + fullfile, outroot, srcbkg, str(ii+1),full_outroot)

         else:
             if docombine==1:
                 if "src" == srcbkg:
                     cs_src_spectra[ii]=1     # was '[ii-1]'
                     sphafiles[ii] = phafile  # was '[ii-1]'

                 elif "bkg" == srcbkg:
                     cs_bkg_spectra[ii]=1     # was '[ii-1]' 
                     bphafiles[ii] = phafile  # was '[ii-1]'

         ###########################
         #
         # create ARF
         #
         ###########################

         if srcbkg=="src" or srcbkg=="bkg" and dobkgresp==1:
             v1("Creating {0} ARF {1}".format(srcbkg,iteminfostr))


             if asol: 
                 (ret_asp, asp_arg, asp_arg_tempfile) = mk_asphist(asol_arg, fullfile, full_outroot, dtf_arg, instrument, chip_id, verbose, clobber, tmpdir)


                 add_tool_history(asp_arg,toolname,pars,toolversion=__revision__)

                 if not ret_asp and verbose != 0:
                     doexit("Failed to create aspect histogram file for " + fullfile, outroot, srcbkg, str(ii+1),full_outroot)


             if instrument == "ACIS":
                 if weight=="yes":
                     (ret, ancrfile, weightfile, fef_file) = create_arf_ext(full_outroot, fullfile, asp_arg, ebin, clobber, verbose, phafile, da_arg, msk_arg, ewmap, bintwmap, pars, tmpdir)

                     add_tool_history(ancrfile,toolname,pars,toolversion=__revision__)

                 else:
                     if "bkg"==srcbkg:
                         # force background to always be weighted
                         (ret, ancrfile, weightfile, fef_file) = create_arf_ext(full_outroot, fullfile, asp_arg, ebin, clobber, verbose, phafile, da_arg, msk_arg, ewmap, bintwmap, pars, tmpdir)

                         add_tool_history(ancrfile,toolname,pars,toolversion=__revision__)

                     elif "src"==srcbkg and correct=="yes":
                         (ret_orig, ancrfile_orig, weightfile) = create_arf_ps(full_outroot, filename, fullfile, asp_arg, ebin, clobber, verbose, phafile, da_arg, msk_arg, chip_id, skyx, skyy, chipx, chipy)   

                         add_tool_history(ancrfile_orig,toolname,pars,toolversion=__revision__)

                         if not ret_orig and verbose != 0:
                             doexit("Failed to create ARF for " + fullfile, outroot, srcbkg, str(ii+1),full_outroot)

                         else:
                             v1("Calculating aperture correction for {0} ARF {1}".format(srcbkg,iteminfostr))

                             (ret, ancrfile) = correct_arf(full_outroot, fullfile, filename, ancrfile_orig, skyx, skyy, binarfcorr, clobber)
                         # a successful run of arfcorr here returns "None"

                     else:    
                         (ret, ancrfile, weightfile) = create_arf_ps(full_outroot, filename, fullfile, asp_arg, ebin, clobber, verbose, phafile, da_arg, msk_arg, chip_id, skyx, skyy, chipx, chipy)
                 
                         add_tool_history(ancrfile,toolname,pars,toolversion=__revision__)
             
             else:
                 # make HRC ARF and copy RMF from CalDB
                 ret, ancrfile, respfile = create_hrc_resp(phafile,refcoord,full_outroot,asp_arg,clobber,verbose,msk_arg,skyx,skyy,instrument,chip_id)
             
                 add_tool_history(ancrfile,toolname,pars,toolversion=__revision__)
                 add_tool_history(respfile,toolname,pars,toolversion=__revision__)

                 if not ret and verbose != 0:
                     doexit("Failed to create ARF for " + fullfile, outroot, srcbkg, str(ii+1),full_outroot)
                             
                 if "src" == srcbkg and correct == "yes":
                     v1("Calculating aperture correction for {0} ARF {1}".format(srcbkg,iteminfostr))
                     ret, ancrfile = correct_arf(full_outroot, fullfile, filename, ancrfile, skyx, skyy, binarfcorr, clobber)


             # NB: If 'ret' is null (b/c mkwarf() fails), the script
             # should terminate here and exit with a mkwarf error.

             if not ret and verbose != 0 and "corr.arf" not in str(ancrfile):
                 doexit("Failed to create ARF for " + fullfile, outroot, srcbkg, str(ii+1),full_outroot)

             else:
                 if docombine==1:
                     if "src" == srcbkg:
                         cs_src_arfs[ii]=1
                         sarffiles[ii] = ancrfile

                     elif "bkg" == srcbkg:
                         cs_bkg_arfs[ii]=1
                         barffiles[ii] = ancrfile    


         ###########################
         #
         # create RMF
         #
         ###########################
             if instrument == "ACIS":
                 v1("Creating {0} RMF {1}".format(srcbkg,iteminfostr))

                 if null_rmffile:
                     v1("WARNING: Setting rmffile parameter (and calquiz calfile) to 'CALDB{}'.\n".format(ccdid_val))

                 rmftool = determine_rmf_tool(phafile, rmffile_ccd, verbose)
                 
                 if weight=="yes":
                     if weight_rmf == "yes":
                         (ret, respfile) = build_rmf_ext(rmftool, ptype, full_outroot, ebin, rmfbin, clobber, verbose, phafile, weightfile)
                     else:
                         (ret, respfile) = build_rmf_ps(rmftool, filename, fullfile, ptype, full_outroot, ebin, rmfbin, clobber, verbose, phafile, weightfile, chip_id, chipx, chipy)

                     add_tool_history(respfile,toolname,pars,toolversion=__revision__)

             # NB: If 'ret' is null (b/c rmf tool fails), 
             # the script should terminate here and exit with a mkacisrmf/mkrmf
             # error.
                 
                 else:
                     (ret, respfile) = build_rmf_ps(rmftool, filename, fullfile, ptype, full_outroot, ebin, rmfbin, clobber, verbose, phafile, weightfile, chip_id, chipx, chipy)

                     add_tool_history(respfile,toolname,pars,toolversion=__revision__)

                 if not ret and verbose != 0:
                     doexit("Failed to create RMF for " + fullfile, outroot, srcbkg, ii+1)

                 else:
                     if docombine==1:
                         if "src" == srcbkg:
                             cs_src_rmfs[ii]=1
                             srmffiles[ii] = respfile 

                         elif "bkg" == srcbkg:
                             cs_bkg_rmfs[ii]=1
                             brmffiles[ii] = respfile 


         ###########################
         #
         # optionally group spectrum
         #
         ###########################
         if "bkg" == srcbkg:
             if 1 == bgdogroup:

                 v1("Grouping {0} spectrum {1}".format(srcbkg,iteminfostr))

                 (ret, grpfile) = group_spectrum(ptype, full_outroot, bggval, bgbinspec, bggtype, clobber, verbose, phafile)

                 # NB: If 'ret' is null (b/c dmgroup() fails),
                 #     the script should terminate here and exit with a dmgroup
                 #     error.

                 if not ret and verbose != 0:  
                    doexit("Failed to group spectrum for " + fullfile, outroot, srcbkg, str(ii+1), full_outroot)


         elif "src" == srcbkg:
             if 1 == dogroup:

                 v1("Grouping {0} spectrum {1}".format(srcbkg,iteminfostr))

                 (ret, grpfile) = group_spectrum( ptype, full_outroot, gval, binspec, gtype, clobber, verbose, phafile)

                 if not ret and verbose != 0: 
                     doexit("Failed to group spectrum for " + fullfile, outroot, srcbkg, str(ii+1), full_outroot)


         ###########################
         #
         # add header keys
         #
         ###########################

         try:
             if srcbkg == "bkg" and dobkgresp == 0:
                 if refcoord != "" and  params["bkgresp"] == "yes":
                     v1("Updating header of {} with RESPFILE and ANCRFILE keywords.\n".format(phafile))
                     
                     edit_headers(verbose, phafile, "RESPFILE", os.path.basename(respfile))
                     edit_headers(verbose, phafile, "ANCRFILE", os.path.basename(ancrfile))

             else:             
                 v1("Updating header of {} with RESPFILE and ANCRFILE keywords.\n".format(phafile))

                 edit_headers(verbose, phafile, "RESPFILE", os.path.basename(respfile))
                 edit_headers(verbose, phafile, "ANCRFILE", os.path.basename(ancrfile))

                 #
                 # If the source or background spectrum was grouped, add the respfile
                 #   and ancrfile keys there, too.
                 #

                 if "bkg" == srcbkg:
                     add_tool_history(phafile,toolname,pars,toolversion=__revision__)

                     if 1 == bgdogroup:

                         v1("Updating header of {} with RESPFILE and ANCRFILE keywords.\n".format(grpfile))

                         edit_headers(verbose, grpfile, "RESPFILE", os.path.basename(respfile))
                         edit_headers(verbose, grpfile, "ANCRFILE", os.path.basename(ancrfile))

                 elif "src" == srcbkg:
                     if 1 == dogroup:

                         v1("Updating header of {} with RESPFILE and ANCRFILE keywords.\n".format(grpfile))

                         edit_headers(verbose, grpfile, "RESPFILE", os.path.basename(respfile))
                         edit_headers(verbose, grpfile, "ANCRFILE", os.path.basename(ancrfile))

         finally:
             if dobkgresp == 0:
                 add_tool_history(phafile,toolname,pars,toolversion=__revision__)

                 if dogroup == 1:
                     add_tool_history(grpfile,toolname,pars,toolversion=__revision__)

##          if dobkgresp == 1:
##              v1("Updating header of " + phafile +
##                       " with RESPFILE and ANCRFILE keywords.")

##              edit_headers(verbose, phafile, "RESPFILE", os.path.basename(respfile))
##              edit_headers(verbose, phafile, "ANCRFILE", os.path.basename(ancrfile))

##              #
##              # If the source or background spectrum was grouped, add the respfile
##              #   and ancrfile keys there, too.
##              #

##              if "bkg" == srcbkg:
##                  add_tool_history(phafile,toolname,pars,toolversion=__revision__)
                 
##                  if 1 == bgdogroup:

##                      v1("Updating header of " + grpfile +
##                        " with RESPFILE and ANCRFILE keywords.")

##                      edit_headers(verbose, grpfile, "RESPFILE", os.path.basename(respfile))
##                      edit_headers(verbose, grpfile, "ANCRFILE", os.path.basename(ancrfile))

##              elif "src" == srcbkg:
##                  if 1 == dogroup:

##                      v1("Updating header of " + grpfile +
##                        " with RESPFILE and ANCRFILE keywords.")

##                      edit_headers(verbose, grpfile, "RESPFILE", os.path.basename(respfile))
##                      edit_headers(verbose, grpfile, "ANCRFILE", os.path.basename(ancrfile))

##          else:
##              if "src" == srcbkg:
##                  v1("Updating header of " + phafile +
##                     " with RESPFILE and ANCRFILE keywords.")
                 
##                  edit_headers(verbose, phafile, "RESPFILE", os.path.basename(respfile))
##                  edit_headers(verbose, phafile, "ANCRFILE", os.path.basename(ancrfile))

##                  if dogroup == 1:
##                      v1("Updating header of " + grpfile +
##                         " with RESPFILE and ANCRFILE keywords.")

##                      edit_headers(verbose, grpfile, "RESPFILE", os.path.basename(respfile))
##                      edit_headers(verbose, grpfile, "ANCRFILE", os.path.basename(ancrfile))
             
##              add_tool_history(phafile,toolname,pars,toolversion=__revision__)

##              if dogroup == 1:
##                  add_tool_history(grpfile,toolname,pars,toolversion=__revision__)

         if "bkg" == srcbkg:

         #
         #  Add the backfile key to the ungrouped source spectrum;
         #  use the ungrouped background spectrum filename.
         #

             if 1 == isoutstack:
                 outdir, outhead = utils.split_outroot(out_stk[ii])

                 if outhead == "":
                     full_outroot = outdir + "src" #stk_read_num(out_stk, ii) + "_bkg"

                 else:
                     full_outroot = outdir + outhead.rstrip("_")

             else:
                 outdir, outhead = utils.split_outroot(outroot)

                 if outhead == "":
                     full_outroot = outdir + "src" + str(ii+1)
                 else:
                     full_outroot = outdir + outhead + "src" + str(ii+1)



             sourcefile  = full_outroot+"."+ptype.lower()
             src_grpfile = full_outroot+ "_grp."+ptype.lower()

             v1("Updating header of {} with BACKFILE keyword.\n".format(sourcefile))
             edit_headers(verbose, sourcefile, "BACKFILE", os.path.basename(phafile))

             add_tool_history(sourcefile,toolname,pars,toolversion=__revision__)

             #
             #  If the source spectrum was grouped,
             #  add the backfile key to the grouped source spectrum.
             #

             if 1 == dogroup:

                 #  If the background is grouped,
                 #   use the grouped background spectrum filename.

                 if 1 == bgdogroup:

                     v1("Updating header of {} with BACKFILE keyword.\n".format(src_grpfile))
                     edit_headers(verbose, src_grpfile, "BACKFILE", os.path.basename(grpfile))

                 else:

                     #  If the background is not grouped,
                     #   use the ungrouped background spectrum filename.

                     v1("Updating header of {} with BACKFILE keyword.\n".format(src_grpfile))
                     edit_headers(verbose, src_grpfile, "BACKFILE", os.path.basename(phafile))

                 add_tool_history(src_grpfile,toolname,pars,toolversion=__revision__)
                 
             # end if dogroup

         # end if bkg

     # end for ii loop

 # end foreach otype loop


         ###################################################################
         #
         # close temporary files that may have been created
         #
         ###################################################################
         try:
             outreg.close()
         except NameError:
             pass

         try:
             fef_file.close()
         except NameError:
             pass

         try: 
             asp_arg_temp.close()
         except NameError:
             pass


 ###################################################################
 #
 # Combine output spectra and responses if the 'docombine' flag has
 # been set and all appropriate files were successfully created.
 #
 ###################################################################


 if sum(cs_src_spectra)==src_count and sum(cs_src_arfs)==src_count and sum(cs_src_rmfs)==src_count:


     if out_count > 1:

             # Define a time stamp variable to use in the 'outroot' parameter of
             # combine_spectra for the case where "combine=yes" and the specextract
             # 'outroot' parameter contains a stack.

         # tstamp_full = time.gmtime()  
         # tstamp_shortstr = str(tstamp_full[0])+str(tstamp_full[1])+str(tstamp_full[2])+"_"+str(tstamp_full[3])+str(tstamp_full[4])

         outroot_cs = out_stk[0] #stk_read_num(out_stk, 1) #tstamp_shortstr

     else:
         outroot_cs  =  outroot

     combine_spectra.punlearn()

     combine_spectra.outroot     = "{}_combined".format(outroot_cs)
     combine_spectra.clobber     = clobber
     combine_spectra.verbose     = verbose
     combine_spectra.src_spectra = ",".join(sphafiles)
     combine_spectra.src_arfs    = ",".join(sarffiles)
     combine_spectra.src_rmfs    = ",".join(srmffiles)

     if sum(cs_bkg_spectra)==src_count: 

         combine_spectra.bkg_spectra = ",".join(bphafiles)

         if dobkgresp == 1:
             if sum(cs_bkg_arfs)==src_count and sum(cs_bkg_rmfs)==src_count:

                 combine_spectra.bkg_arfs    = ",".join(barffiles)
                 combine_spectra.bkg_rmfs    = ",".join(brmffiles)

                 combine_spectra()
                 v2("Combined source and background spectra and responses.")

         else:
             combine_spectra()
             v2("Combined source spectra and responses, and background spectra.")

     else:

         combine_spectra()
         v2("Combined source spectra and responses.")


 else:
     if src_count > 1 and combine=="yes":
         v1("Output spectra and responses were not combined because spectra and/or responses were not created for every item in the input stack(s) of files.")


     #else:
     #    v2("Spectra and responses were not combined.")

##  finally:
##      # close tempfiles
##      if False not in [asp.startswith("@"), params["asp"] == "", params["asp"].lower() != "none"]:
##          os.remove(temp_asol_stk.name)

##      if False not in [pbkfile.startswith("@"), params["pbkfile"] == "", params["pbkfile"].lower() != "none"]:
##          os.remove(temp_pbk_stk.name)

##      if False not in [bpixfile.startswith("@"), params["bpixfile"] == "", params["bpixfile"].lower() != "none"]:
##          os.remove(temp_bpix_stk.name)

##      if False not in [mask.startswith("@"), params["mskfile"] == "", params["mskfile"].lower() != "none"]:
##          os.remove(temp_msk_stk.name)

if __name__ == "__main__":
    specextract(sys.argv)
    
quit()













