#!/usr/bin/env python

#
# Copyright (C) 2011, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021
#        Smithsonian Astrophysical Observatory
#
#
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
#

"""
Script:

  specextract

	This script is an enhanced Python translation of the original CIAO tool
	specextract, which was written in S-Lang. It is run from the Unix
	command line, within the CIAO environment.
"""

__version__ = "CIAO 4.13"

toolname = "specextract"
__revision__ = "DD MMM 2021"


##########################################################################
#
# This script requires a parameter file for itself and for its underlying
# tools:  dmextract, mkrmf, mkacisrmf, mkwarf, mkarf, sky2tdet, acis_fef_lookup,
# dmgroup, dmhedit, calquiz, asphist, and acis_set_ardlib.
#
##########################################################################

# Load the necessary libraries.

import paramio, os, sys, shutil, numpy, stk, tempfile, caldb4
import pycrates as pcr

from ciao_contrib.logger_wrapper import initialize_logger, make_verbose_level, set_verbosity, handle_ciao_errors, get_verbosity

from ciao_contrib.param_wrapper import open_param_file

from ciao_contrib.runtool import dmextract, mkrmf, mkacisrmf, mkarf, mkwarf, arfcorr, asphist, dmgroup, dmhedit, calquiz, dmcopy, acis_fef_lookup, sky2tdet, combine_spectra, ardlib, acis_set_ardlib, dmmakereg, add_tool_history, dmhistory, dmimgthresh

import ciao_contrib._tools.fileio as fileio
import ciao_contrib._tools.utils as utils
import ciao_contrib._tools.specextract as spec

from ciao_contrib.runtool import new_pfiles_environment
from sherpa.utils import parallel_map
from ciao_contrib.parallel_wrapper import parallel_pool
from multiprocessing import cpu_count


###############################################


# Set up the logging/verbose code
initialize_logger(toolname)

# Use v<n> to display messages at the given verbose level.
v0 = make_verbose_level(toolname, 0)
v1 = make_verbose_level(toolname, 1)
v2 = make_verbose_level(toolname, 2)
v3 = make_verbose_level(toolname, 3)
v4 = make_verbose_level(toolname, 4)
v5 = make_verbose_level(toolname, 5)



def extract_spectra(full_outroot, infile, ptype, channel, ewmap, binwmap, instrument, clobber, verbose):
    """
    Create spectrum from input file, return spectrum file name
    """

    specfile = f"{full_outroot}.{ptype.lower()}"

    dmextract.punlearn()

    dmextract.outfile = specfile
    dmextract.opt = "pha1"
    dmextract.clobber = clobber
    dmextract.verbose = str(verbose)

    if channel == "1:1024:1":
        dmextract.infile = f"{infile}[bin {ptype}]"
    else:
        dmextract.infile = f"{infile}[bin {ptype}={channel}]"

    if instrument == "ACIS":
        dmextract.wmap = f"[energy={ewmap}][bin {binwmap}]"
    else:
        dmextract.wmap = ""

    dmextract()

    return specfile



def create_arf_ps(full_outroot, evt_filename, infile, asp_param, ebin, clobber, verbose, specfile, dafile, mskfile, ccd_id, skyx, skyy, chipx, chipy, infostr):

    """
    Run mkarf to create an unweighted ARF.

    Use acis_fef_lookup to locate the appropriate FEF file for input
    to mkrmf, which is the FEF-file-finding method to use for creating
    unweighted RMFs.  Return the name of the ARF and FEF file.
    """

    # If file does not have a CTI_APP header keyword, issue
    # a warning that the data should probably be reprocessed.

    v1(infostr)
    
    cti_app_val = fileio.get_keys_from_file(evt_filename)["CTI_APP"]

    if cti_app_val.upper() == "NONE":
        raise IOError(f"File {evt_filename} is missing a CTI_APP header keyword, required by many CIAO tools; an ARF will not be created. Try re-running specextract after reprocessing your data.\n")

    acis_fef_lookup.punlearn()

    acis_fef_lookup.infile = evt_filename
    acis_fef_lookup.chipid = ccd_id
    acis_fef_lookup.chipx  = chipx
    acis_fef_lookup.chipy  = chipy
    acis_fef_lookup.verbose = "0"

    acis_fef_lookup()

    feffile = acis_fef_lookup.outfile


    # generate mkarf 'detsubsys' keyword
    ccdid_mode_val = int(ccd_id)

    if ccdid_mode_val > 3:
        local_id = ccdid_mode_val - 4
        detname = f"ACIS-S{local_id}"
    else:
        local_id = ccdid_mode_val
        detname = f"ACIS-I{local_id}"

    # ARF output file
    arffile = f"{full_outroot}.arf"

    mkarf.punlearn()

    mkarf.detsubsys = f"{detname};BPMASK=0x03ffff"
    mkarf.outfile = arffile
    mkarf.asphistfile = asp_param
    mkarf.sourcepixelx = skyx
    mkarf.sourcepixely = skyy
    mkarf.grating = fileio.get_keys_from_file(evt_filename)["GRATING"]
    mkarf.obsfile = evt_filename
    mkarf.dafile = dafile
    mkarf.maskfile = mskfile
    mkarf.verbose = str(verbose)
    mkarf.engrid = ebin
    mkarf.clobber = clobber

    mkarf()
    
    return arffile, feffile



def create_arf_ext(full_outroot, infile, asp_param, ebin, clobber, verbose, specfile, dafile, mskfile, ewmap_param, bintwmap_param, wmap_clip, wmap_thresh, pars, tmpdir, infostr):
    
    """
    Run mkwarf to create a weighted ARF.  
    
    Input the sky2tdet WMAP to create
    an ARF and weight file (to be used for creating a RMF, if using mkrmf).
    Return the names of the ARF, weights, and TDET WMAP files
    """
	
    tdetwmap = tempfile.NamedTemporaryFile(suffix="_tdet",dir=tmpdir)

    v1(infostr)
    
    try:
        try:
            sky2tdet.punlearn()

            sky2tdet.infile = f"{infile}[energy={ewmap_param}][bin sky]" # include extraction region
                                                                         # plus same energy filter
            
            # used for dmextract wmap input to mkacisrmf
            sky2tdet.bin = bintwmap_param
            sky2tdet.asphistfile = asp_param
            sky2tdet.outfile = f"{tdetwmap.name}[wmap]"
            sky2tdet.clobber = "yes"
            sky2tdet.verbose = verbose

            sky2tdet()

            # genearte clipped WMAP to speed up response generation
            if wmap_clip:
                clip_wmap(tdetwmap.name,wmap_thresh,tmpdir)

        except (OSError,IOError):
            raise IOError("Error generating WMAP w/sky2tdet!")
			
        try:
            # ARF output file
            arffile = f"{full_outroot}.arf"
            
            # output weight file used in mkrmf
            weightfile = tempfile.NamedTemporaryFile(suffix=".wfef",dir=tmpdir)
            
            mkwarf.punlearn()

            mkwarf.infile = f"{tdetwmap.name}[wmap]"
            mkwarf.outfile = arffile
            mkwarf.weightfile = weightfile.name
            mkwarf.mskfile = mskfile
            mkwarf.dafile = dafile
            mkwarf.spectrumfile = ""
            mkwarf.egridspec = ebin
            mkwarf.detsubsysmod = "BPMASK=0x03ffff"
            mkwarf.clobber = "yes"
            mkwarf.verbose = str(verbose)

            mkwarf()
			
        except (OSError,IOError):
            raise IOError("Error generating weighted ARF w/mkwarf!")
			
    except IOError as err_msg:
        print(err_msg)

        raise IOError(f"Failure to create weighted ARF.  Possible causes include: zero counts in the input region; a memory allocation error; or corrupt ARDLIB.  Try running {toolname} with weight=no, binarfmap!=1, or punlearn ardlib.")

    return arffile, weightfile.name, weightfile, tdetwmap



def correct_arf(full_outroot, infile, evt_filename, orig_arf, skyx, skyy, binarfcorr, clobber):
    """
    Apply an energy-dependent point-source aperture correction
    to the source ARF created by mkarf, if user has set the
    'correct' specextract parameter to 'yes' and weight is 'no'.
    It is not appropriate to run arfcorr on background ARFs because
    background is extended.

    Return the name of the corrected ARF file.
    """
    
    gz_input = os.path.exists(evt_filename+".gz")
    guz_input = os.path.exists(evt_filename)

    if not infile.endswith(".gz") and not guz_input:

        if gz_input:
            evt_filename_gz = f"{evt_filename}.gz"

            infile = infile.replace(evt_filename, evt_filename_gz)

            v1("NOTE: {0} does not exist, but {0}.gz does; using the latter as input to arfcorr for the ARF correction.\n".format(evt_filename))

    # arfcorr required input image:
    reg_image  = f"{infile}[bin sky={binarfcorr}]"

    # ARF output file
    carffile = f"{full_outroot}.corr.arf"

    arfcorr.punlearn()
    arfcorr.infile = reg_image
    arfcorr.arf = orig_arf
    arfcorr.outfile = carffile
    arfcorr.region = spec.get_region_filter(infile)[1]
    arfcorr.x = skyx
    arfcorr.y = skyy
    arfcorr.energy = 0
    arfcorr.verbose = 0
    arfcorr.clobber = "yes"

    arfcorr()

    # add arfcorr history to file, since it doesn't automatically
    param_arfcorr = {}
    for acpar in arfcorr._parnames:
        param_arfcorr[acpar] = arfcorr._get_param_value(acpar)

    add_tool_history(carffile,"arfcorr",param_arfcorr)
    
    return carffile



def create_hrc_resp(specfile,rmf_file,refcoord,full_outroot,asp_param,clobber,verbose,mskfile,skyx,skyy,instrument,chip_id,infostr):
    """
    Generate HRC ARF with mkarf and copy RMF from CalDB
    """
    
    # use the 'caldb4' module to query for the appropriate,
    # latest HRC RMF in CALDB;  can use calquiz instead as well,
    # output is a string rather than list

    if rmf_file.upper() == "CALDB":
        # cf. https://cxc.cfa.harvard.edu/cal/Hrc/detailed_info.html#rmf for HRC RMF information
 
        rmf = caldb4.Caldb(infile=specfile,product="MATRIX").search

        if len(rmf) == 0:
            raise IOError(f"{specfile} is an invalid HRC spectral file, as non-SAMP responses are no longer supported by the CALDB.  Please reprocess the dataset or re-download the observation from the archive.")

        if len(rmf) > 1:
            # this should not happen
            raise IOError("Multiple HRC RMFs returned by CALDB: {}".format(",".join(rmf)))

        rmf = rmf[0]
        rmf = rmf[:rmf.find("[")]

        # copy RMF
        rmffile = f"{full_outroot}.rmf"
        shutil.copyfile(rmf,rmffile)

    else:
        # enable RMFFILE parameter for HRC data to support HRC Cal group
        v1("Warning: 'rmffile' parameter for HRC observations is meant for Calibration Group and expert usage!")

        rmffile = rmf_file

    # establish detsubsys       
    if instrument == "HRC":
        if chip_id == "0":
            detname = "HRC-I"
        else:
            detname = f"HRC-S{chip_id}"

    # ARF output file
    v1(infostr)
    
    arffile = f"{full_outroot}.arf"

    mkarf.punlearn()

    mkarf.detsubsys = detname
    mkarf.outfile = arffile
    mkarf.asphistfile = asp_param
    mkarf.sourcepixelx = skyx
    mkarf.sourcepixely = skyy
    mkarf.grating = fileio.get_keys_from_file(specfile)["GRATING"]
    mkarf.obsfile = specfile
    mkarf.maskfile = mskfile
    mkarf.verbose = str(verbose)
    mkarf.clobber = clobber

    try:
        mkarf.engrid = f"grid({rmffile}[SPECRESP MATRIX][cols ENERG_LO,ENERG_HI])"
        mkarf()

    except OSError:        
        mkarf.engrid = f"grid({rmffile}[MATRIX][cols ENERG_LO,ENERG_HI])"
        mkarf()
            
    return arffile, rmffile



def determine_rmf_tool(infile,rmffile,verbose):
    """
    Decide whether to run mkrmf or mkacisrmf, return the name of the tool to run.
    """
    
    caldb_p2_resp_file = ""   # P2_RESP CALDB file

    # NB: Adjust call_calquiz() function eventually
    #     to avoid this step of having to initialize
    #     the variable.

    # rmftool               # rmf tool to use: 'mkrmf' or 'mkacisrmf'
    # numCalFiles           # number of CALDB files found by calSearch

    # query the CALDB for the p2_resp file

    v2("Searching for P2_RESP calibration file...")

    calquiz_results = call_calquiz(infile,"SC_MATRIX",rmffile,caldb_p2_resp_file,verbose)
    numCalFiles = calquiz_results[0]

    caldb_p2_resp_file = calquiz_results[1]

    v3(f"Found the following P2_RESP file(s): {caldb_p2_resp_file}")

    if not numCalFiles or numCalFiles in [0.0, 0, 0.]:

        v1("Cannot use mkacisrmf because no P2_RESP files were found.\n")

        v1(f"Please reprocess {infile} with acis_process_events if you wish to use mkacisrmf, unless the focal plane temperature is greater than -110C and the observation is taken in graded-mode or the extraction region is on a front-illuminated CCD.\n")

        v1("Using mkrmf...\n")

        rmftool = "mkrmf"

    else:

        v1("Using mkacisrmf...\n")

        rmftool = "mkacisrmf"

    return rmftool



def build_rmf_ps(rmftool, evt_filename, infile, ptype, full_outroot, ebin, rmfbin, clobber, verbose, specfile, weightfile, ccd_id, chipx, chipy, infostr):
	
    """
    Run either mkrmf or mkacisrmf depending on input conditions.
    For mkrmf: input FEF file and no weights file and return RMF name.
    For mkacisrmf: input CALDB and WMAP='none' and return RMF name.
    
    Returns unweighted ACIS RMF filename.
    """

    v1(infostr)
    
    rmffile = f"{full_outroot}.rmf"

    try:
        if rmftool == "mkrmf":
            acis_fef_lookup.punlearn()

            acis_fef_lookup.infile = evt_filename
            acis_fef_lookup.chipid = ccd_id
            acis_fef_lookup.chipx= chipx
            acis_fef_lookup.chipy = chipy
            acis_fef_lookup.verbose = str(0)
            acis_fef_lookup()

            mkrmf.punlearn()

            mkrmf.infile = acis_fef_lookup.outfile
            mkrmf.outfile = rmffile
            mkrmf.logfile = ""
            mkrmf.weights = ""
            mkrmf.axis1 = f"energy={ebin}"
            mkrmf.axis2 = rmfbin
            mkrmf.clobber = clobber
            mkrmf.verbose = str(verbose)

            mkrmf()

        else:
            channel = rmfbin.split("=")
            channel.reverse()

            mkacisrmf.punlearn()

            # force CALDB querry to match 'ccd_id' parameter, otherwise the default
            # behavior is to use the CCD_ID header keyword in the 'obsfile'
            mkacisrmf.infile = f"CALDB(CCD_ID={ccd_id})"
            mkacisrmf.outfile = rmffile
            mkacisrmf.energy = ebin
            mkacisrmf.channel = channel[0]
            mkacisrmf.chantype = ptype.upper()
            mkacisrmf.wmap = "none"
            mkacisrmf.gain = "CALDB"
            mkacisrmf.obsfile = evt_filename
            mkacisrmf.ccd_id = ccd_id
            mkacisrmf.chipx = chipx
            mkacisrmf.chipy = chipy
            mkacisrmf.clobber = clobber
            mkacisrmf.verbose = str(verbose)

            mkacisrmf()

        return rmffile

    except (OSError,IOError):
        raise IOError(f"Failed to find an appropriate tool to generate a RMF for {specfile}")

    

def build_rmf_ext(rmftool, ptype, full_outroot, ebin, rmfbin, clobber, verbose, specfile, weightfile, wmap_clip, wmap_sky2tdet, infostr):

    """
    Run either mkrmf or mkacisrmf depending on input conditions. 
    For mkrmf: input CALDB and weight file and return RMF name.
    For mkacisrmf: input CALDB and WMAP and return RMF name.

    Returns weighted ACIS RMF filename.
    """

    v1(infostr)
    
    rmffile = f"{full_outroot}.rmf"

    try:
        if rmftool == "mkrmf":

            mkrmf.punlearn()

            mkrmf.infile = "CALDB"
            mkrmf.outfile = rmffile
            mkrmf.logfile = ""
            mkrmf.weights = weightfile
            mkrmf.axis1 = f"energy={ebin}"
            mkrmf.axis2 = rmfbin
            mkrmf.clobber = clobber
            mkrmf.verbose = str(verbose)

            mkrmf()

        else:
            channel = rmfbin.split("=")
            channel.reverse()

            mkacisrmf.punlearn()

            mkacisrmf.infile = "CALDB"
            mkacisrmf.outfile = rmffile
            mkacisrmf.energy = ebin
            mkacisrmf.channel = channel[0]
            mkacisrmf.chantype = ptype.upper()
            mkacisrmf.gain = "CALDB"
            mkacisrmf.clobber = clobber
            mkacisrmf.verbose = str(verbose)
            mkacisrmf.ccd_id = "0" # hopefully, this par will
                                   # be ignored since it can't
                                   # be set to "".
            if wmap_clip:
                mkacisrmf.wmap = f"{wmap_sky2tdet}[wmap]"
            else:
                mkacisrmf.wmap = f"{specfile}[WMAP]" # dmextract WMAP is faster than sky2tdet WMAP

            mkacisrmf()

        return rmffile

    except (OSError,IOError):
        raise IOError(f"Failed to find an appropriate tool to generate a RMF for {specfile}")


    
def set_badpix(evtfile, bpixfile, instrument, verbose):
    """
    set a different bad pixel file in ARDLIB for each
    observation in an input source or background stack   
    """
    
    ardlib.read_params()

    if instrument == "ACIS":
        bpixfile = ",".join(stk.build(bpixfile))

        acis_set_ardlib.punlearn()

        acis_set_ardlib.badpixfile = bpixfile
        acis_set_ardlib.verbose = verbose

        acis_set_ardlib()
        ardlib.write_params()

        try:
            acis_set_ardlib()
            #print(f"Updated ARDLib: {ardlib}") # verify for testing
        except (OSError,IOError):
            raise IOError(f"Failed to set {bpixfile} bad pixel file in ardlib.par for {evtfile}.")

    else:
        bpixpar = "AXAF_{}_BADPIX_FILE".format(fileio.get_keys_from_file(evtfile)["DETNAM"])
        bpix = f"{bpixfile}[BADPIX]"

        try:
            setattr(ardlib,bpixpar.replace("-","_"),bpix)
            ardlib.write_params()

        finally:
            if not os.path.isfile(bpixfile) or getattr(ardlib,bpixpar.replace("-","_")).rstrip("[BADPIX]") != bpixfile:
                raise IOError(f"Failed to set {bpixfile} bad pixel file in ardlib.par for {evtfile}.")


            
def convert_region(infile, evt_filename, outfile, clobber, verbose):
    """
    Convert user-input source/background regions to physical coordinates, in
    order to ensure that regions input to arfcorr via the correct_arf() function
    are in the correct form. Running this function on regions already in
    physical coordinates has no effect other than converting CIAO region
    format to DS9 format.
    """
    ## Output region name
	
    dmmakereg.punlearn()

    dmmakereg.region  = spec.get_region_filter(infile)[1]
    dmmakereg.outfile = outfile
    dmmakereg.wcsfile = evt_filename
    dmmakereg.kernel  = "fits"
    dmmakereg.clobber = clobber
    dmmakereg.verbose = verbose

    dmmakereg()

    return infile.replace(spec.get_region_filter(infile)[1], f"region({outfile})")



def mk_asphist(asol_param, evt_filename_filter, full_outroot, dtffile, instrument, chip_id, verbose, clobber, tmpdir):
    """
    Run asphist to create one aspect histogram file per user-input source
    observation for input into sky2tdet ('weight=yes') or mkarf ('weight=no');
    the assumption is that it is appropriate to analyze each observation using
    only one, and not multiple, aspect histogram files.

    Return per chip aspect histogram and file name string
    """

    asp_out = tempfile.NamedTemporaryFile(suffix=f"_asphist{chip_id}",dir=tmpdir)

    asphist.punlearn()

    asphist.infile = asol_param # single file, @files.lis, or
                                # comma-separated list of files

    if instrument == "ACIS":
        asphist.evtfile = f"{evt_filename_filter}[ccd_id={chip_id}]"
        asphist.dtffile = ""
    else:
        asphist.evtfile = f"{evt_filename_filter}[chip_id={chip_id}]"
        asphist.dtffile = dtffile

    asphist.outfile = asp_out.name
    asphist.verbose = verbose
    asphist.clobber = "yes"

    asphist()
    
    return asp_out, asp_out.name



def pi_alternate_wmap(pi,wmap):
    """
    replace the WMAP in the PI file (typically the dmextract generated version) 
    with an alternate WMAP (typically generated by sky2tdet).
    """

    tmp_spec = tempfile.NamedTemporaryFile(suffix="PI-WMAP",dir=tmpdir)    
    tmp_wmap = tempfile.NamedTemporaryFile(suffix="wmap",dir=tmpdir)
    
    cr = pcr.read_file(pi,mode="rw")

    ds = cr.get_dataset()
    ds.delete_crate("WMAP")
    ds.write(tmp_spec.name,clobber=True)

    del(cr)
    del(ds)

    ## this approach appends WMAP to the last block
    # dmappend.punlearn()
    # dmappend.infile = f"{wmap}[WMAP][subspace -time]"
    # dmappend.outfile = pi
    # dmappend.verbose = "0"
    # dmappend()
    
    cr_wmap = pcr.read_file(f"{wmap}[WMAP][subspace -time]")

    ds_wmap = cr_wmap.get_dataset().get_crate("WMAP")
    ds_wmap.write(tmp_wmap.name,clobber=True)

    del(cr_wmap)
    del(ds_wmap)

    dmcopy.punlearn()
    dmcopy.infile = tmp_wmap.name
    dmcopy.outfile = pi
    dmcopy.clobber = "yes"
    dmcopy.verbose = "0"
    dmcopy()
    
    dmappend.punlearn()
    dmappend.infile = tmp_spec.name
    dmappend.outfile = pi
    dmappend.verbose = "0"
    dmappend()

    tmp_spec.close()
    tmp_wmap.close()
    

    
def clip_wmap(wmapfile,threshold,tmpdir):
    """
    OPTIONAL: Truncate sky2tdet WMAP to speed up mkwarf by threshold 
              clipping the WMAP for large areas. Rebinning the WMAP 
              is less desirable since it essentially can randomly cause 
              badpixels/columns/etc to be over or under weighted.  (It 
              will be the same every time, just random in that you don't 
              have control over it.)  If your region is huge (i.e. whole 
              chip) then that probably won't matter, if it's a modest size, 
              eg off-axis point-like source, then binning is bad.
    
              One way to threshold (and there are several) is just to look 
              at the pixel values in the WMAP, and determine a cutoff that 
              preserves a certain amount of the total flux. That way, if a 
              badpixel/column/etc. is not weighted by a lot of flux it can 
              be explicitly purged and likewise if they are covered by a 
              large flux, then they will be preserved and the ARF 
              appropriately weighted.
    """

    cr = pcr.read_file(f"{wmapfile}[wmap]")
    im = cr.get_image().values
    threshold = float(threshold)

    del(cr)

    # sort pixel values

    im_sort = im.flatten()
    im_sort.sort()

    im, = numpy.where(im_sort > 0)
    im_sort = im_sort[im]

    # create cumulative flux distribution

    flux_dist = im_sort.cumsum()
    flux_dist /= flux_dist[-1] # normalize flux distribution

    flux_dist_thresh, = numpy.where(flux_dist < threshold)
    cutoff = im_sort[flux_dist_thresh[-1]]

    # Give some feedback on statistics of applying threshold

    flux_dist_info = numpy.arange(len(flux_dist)*1.0)
    flux_dist_info /= flux_dist_info[-1]
    perc = int(flux_dist_thresh[-1] / (len(flux_dist)*0.01)+0.5)
    cutoff_stat = f"{threshold*100:.3g}% flux cutoff @{cutoff:.3g} removes {perc}% area\n" #.format((threshold*100),cutoff,perc)
    v1(cutoff_stat)      

    # Run dmimgthresh with 'cutoff'

    threshfile = tempfile.NamedTemporaryFile(dir=tmpdir)

    dmcopy.punlearn()
    dmcopy.infile = wmapfile
    dmcopy.outfile = threshfile.name
    dmcopy.verbose = "0"
    dmcopy.clobber = "yes"

    dmcopy()       

    dmimgthresh.punlearn()

    dmimgthresh.infile = threshfile.name
    dmimgthresh.outfile = wmapfile 
    dmimgthresh.cut = cutoff
    dmimgthresh.value = "0.0"
    dmimgthresh.expfile = ""
    dmimgthresh.clobber = "yes"
    dmimgthresh.verbose = "0"

    dmimgthresh()

    threshfile.close()
    del(im)
    del(im_sort)
    del(flux_dist)
    del(flux_dist_thresh)
        

    
def group_spectrum(ptype, full_outroot, val, spec, gtype,
                   clobber, verbose, phafile):
    """
    Optionally group output spectrum
    """

    
    # grouped spectrum name
    grpout = f"{full_outroot}_grp.{ptype.lower()}"

    dmgroup.punlearn()

    dmgroup.infile = f"{phafile}[SPECTRUM]"
    dmgroup.outfile = grpout
    dmgroup.binspec = spec
    dmgroup.grouptype = gtype
    dmgroup.grouptypeval = val
    dmgroup.ycolumn = "counts"
    dmgroup.xcolumn = "channel"
    dmgroup.tabcolumn = ""
    dmgroup.clobber = clobber
    dmgroup.verbose = verbose

    dmgroup()
    
    return grpout



def edit_headers(verbose, infile, key, val, *args, **kwargs):
    """
    Update or add the infile header keyword with a specified value.
    """
    
    unit = kwargs.get("unit",None) # optional argument
    comment = kwargs.get("comment",None)

    dmhedit.punlearn()

    dmhedit.infile = infile
    dmhedit.filelist = "none"
    dmhedit.operation = "add"
    dmhedit.key = key
    dmhedit.value = str(val)
    dmhedit.verbose = str(verbose)

    if unit is not None:
        dmhedit.unit  = str(unit)

    if comment is not None:
        dmhedit.comment = str(comment)

    return dmhedit()



def call_calquiz(infile, product, query, file, verbose):
    """
    Retrieve the CalDB file name and number of files found by
    calling calquiz as a list
    """
    
    #clear params and set calquiz command string
    calquiz.punlearn()

    calquiz.infile  =  f"{infile}[WMAP]"
    calquiz.product =  product
    calquiz.calfile =  query
    calquiz.outfile =  "y"
    calquiz.echo    = "yes"
    calquiz.verbose = verbose

    # run calquiz cmd, pget outfile if successful
    result = calquiz()   # can try calquiz.outfile here and perhaps
                         # remove "y" in outfile

    if [result != "", "ERROR" in str(result)] == [True,False]:
        cqpf = paramio.paramopen("calquiz", "r")

        if not cqpf:
            raise IOError("Unable to open calquiz parameter file.")

        else:
            outfiles = paramio.pgetstr(cqpf, "outfile")

            outfiles_arr = numpy.array(outfiles.split(","))

            dims = outfiles_arr.shape
            num_dims = outfiles_arr.ndim
            data_type = outfiles_arr.dtype.name

            if 0 == len(outfiles_arr):
                numFiles = 0
                file = ""

            else:
                # parse comma-separated outfile list to array

                file = outfiles_arr[0]
                numFiles = len(outfiles_arr)
                paramio.paramclose(cqpf)

    else:
        numFiles = 0
        file = None

    return [numFiles, file]


##########################################################################
# 
#  Parallelizable functions; main steps for spectral extraction 
#  and response generation.  Dependent on above functions  
#
##########################################################################

def spectra(args):
    key_id = args["key_id"]
    srcbkg = args["srcbkg"]
    fullfile = args["fullfile"]
    filename = args["filename"]
    full_outroot = args["full_outroot"]
    dobpix = args["dobpix"]
    instrument = args["instrument"]
    weight = args["weight"]
    correct = args["correct"]
    binwmap = args["binwmap"]    
    ptype = args["ptype"]
    channel = args["channel"]
    ewmap = args["ewmap"]
    tmpdir = args["tmpdir"]
    clobber = args["clobber"]
    verbose = args["verbose"]
    fcount = args["fcount"]
    iteminfostr = args["iteminfostr"]

    try:
        bpix = args["bpix"]
    except KeyError:
        pass

    try:
        nrao_nh = args["nrao_nh"]
    except KeyError:
        pass

    try:
        bell_nh = args["bell_nh"]
    except KeyError:
        pass
    
    with new_pfiles_environment(ardlib=True):
        ###################################
        #
        # set bad pixel file in ardlib.par
        #
        ###################################

        if dobpix:
            v1(f"Setting bad pixel file {iteminfostr}")
            set_badpix(filename, bpix, instrument, verbose)

        #####################################
        #
        # convert source region to physical
        # coordinates for ARF correction
        #
        #####################################

        if weight != "yes":
            if correct == "yes" and srcbkg == "src":
                if not spec.get_region_filter(fullfile)[0]:
                    v0(f"WARNING: The ARF generated for {fullfile} cannot be corrected as no supported spatial region filter was detected for this file, which is required input for this step.\n")

                    correct = "no"

                    pardict = {key_id : {"correct" : correct}}
                    
                else:
                    v1(f"Converting source region to physical coordinates {iteminfostr}")

                    outreg = tempfile.NamedTemporaryFile(suffix=f"_phys_coords_{key_id}.reg",dir=tmpdir)
                    fullfile = convert_region(fullfile,filename,outreg.name,"yes",verbose)

                    #pardict = {key_id : {"fullfile" : fullfile, "outreg" : outreg}}
                    
        ###########################
        #
        # extract spectrum
        #
        ###########################

        v1(f"Extracting {srcbkg} spectra {iteminfostr}")

        # If 'binwmap' contains 'tdet' string, check that TDET column exists
        # in file; if not, change 'binwmap' value to
        # 'det={user's specification}' and notify user.

        if "tdet" in binwmap:
            cr = pcr.read_file(fullfile)

            if cr is None:
                raise IOError(f"Unable to read from file {fullfile}")

            if not cr.column_exists("tdet"):
                v1(f"WARNING: No TDET column found in {filename}; the 'wmap' parameter of dmextract will be set to use DET coordinates instead.\n")

                binwmap_val = binwmap.split("=")[1]
                binwmap = f"det={binwmap_val}"

                del(cr)

        try:
            phafile = extract_spectra(full_outroot,fullfile,ptype,channel,
                                      ewmap,binwmap,instrument,clobber,verbose)

            try:
                pardict[key_id].update({"phafile" : phafile})
            except NameError:
                pardict = {key_id : {"phafile" : phafile}}
                
        except OSError:
            raise IOError(f"Failed to extract spectrum for {fullfile}")

        # add nH values to phafile header
        try:
            # Galactic neutral hydrogen column density at the source position using
            # the NRAO all-sky interpolation, via COLDEN.

            edit_headers(verbose,phafile,"NRAO_nH",float(f"{nrao_nh:.6}"),unit="10**22 cm**-2",
                         comment="galactic HI column density") 
        except NameError:
            pass
            
        try:
            # Galactic neutral hydrogen column density at the source position using
            # the Bell Labs survey, via COLDEN.

            edit_headers(verbose,phafile,"Bell_nH",float(f"{bell_nh:.6}"),unit="10**22 cm**-2",
                         comment="galactic HI column density")
        except NameError:
            pass


        ############################################################
        #
        # add AREASCAL value from 'blanksky' BKGSCAL value, do not
        # change if using 'blanksky_sample'
        #
        ############################################################
        
        if srcbkg == "bkg":
            dmhistory.punlearn()
            dmhistory.infile = fullfile

            try:
                dmhistory.tool = "blanksky"
                bsky_status = dmhistory()
            except OSError:
                bsky_status = None

            try:
                dmhistory.tool = "blanksky_sample"
                bskysamp_status = dmhistory()
            except OSError:
                bskysamp_status = None
                
            if all([bsky_status is not None, bskysamp_status is None]):
                src_cr = pcr.read_file(f"{filename}[#row=1]")
                ccdid = src_cr.get_column("CCD_ID").values[0]
                del(src_cr)

                bkgscale = fileio.get_keys_from_file(fullfile)[f"BKGSCAL{ccdid}"]

                edit_headers(verbose, phafile, "AREASCAL", bkgscale, comment=f"blanksky BKGSCAL{ccdid} scaling factor from the 'blanksky' script, set to 1.0 if subtracting background")

                v1(f"{phafile} AREASCAL keyword set to 'blanksky' BKGSCAL{ccdid} value for automated background spectrum scaling while spectral modeling.  Set AREASCAL to 1.0 if 'blanksky' spectrum is being used for background subtraction.")

    try:
        outreg.close()
    except NameError:
        pass
                
    try:
        return pardict
    except NameError:
        pass

    

def resps(args):
    key_id = args["key_id"]
    srcbkg = args["srcbkg"]
    fullfile = args["fullfile"]
    filename = args["filename"]
    full_outroot = args["full_outroot"]
    phafile = args["phafile"]
    dobpix = args["dobpix"]

    instrument = args["instrument"]
    ebin = args["ebin"]
    ptype = args["ptype"]  
    bintwmap = args["bintwmap"]
    ewmap = args["ewmap"]

    correct = args["correct"]
    skyx = args["skyx"]
    skyy = args["skyy"]
    chip_id = args["chip_id"]
    chipx = args["chipx"]
    chipy = args["chipy"]
    binarfcorr = args["binarfcorr"]

    refcoord = args["refcoord"]

    weight = args["weight"]
    weight_rmf = args["weight_rmf"]
    wmap_clip = args["wmap_clip"]
    wmap_threshold = args["wmap_threshold"]
    rmfbin = args["rmfbin"]
    
    dobkgresp = args["dobkgresp"]

    tmpdir = args["tmpdir"]
    clobber = args["clobber"]
    verbose = args["verbose"]
    iteminfostr = args["iteminfostr"]
    pars = args["pars_specextract"]
    
    try:
        asol = args["asol"]
        asolstat = True
    except KeyError:
        asphist = args["asphist"]
        asolstat = False

    try:
        bpix = args["bpix"]
    except KeyError:
        pass

    try:
        dtf = args["dtf"]
    except KeyError:
        pass

    try:
        da = args["da"]
    except KeyError:
        pass

    try:
        msk = args["msk"]
    except KeyError:
        pass
                
    if instrument == "ACIS":
        null_rmffile = args["null_rmffile"]
        rmffile_ccd = args["rmffile_ccd"]
    else:
        rmffile = args["rmffile"]

    if all([weight == "no", correct == "yes", srcbkg == "src"]):
        outreg = tempfile.NamedTemporaryFile(suffix=f"_phys_coords_{key_id}.reg",dir=tmpdir)
        fullfile = convert_region(fullfile,filename,outreg.name,"yes",verbose)
        
    with new_pfiles_environment(ardlib=True):
        if dobpix:
            set_badpix(filename,bpix,instrument,verbose)

        infostr = f"Creating {srcbkg} ARF {iteminfostr}"

        if asolstat:                
            #########################################
            #
            # create aspect histograms if necessary
            # to pass along toe mkarf
            #
            #########################################
            
            with spec.suppress_stdout_stderr():
                try:
                    asphist_temp, asphist = mk_asphist(asol,fullfile,full_outroot,
                                                       dtf,instrument,chip_id,
                                                       verbose,clobber,tmpdir)

                    add_tool_history(asphist,toolname,pars,toolversion=__revision__)

                except OSError:
                    if dobkgresp:
                        raise IOError(f"Failed to create aspect histogram file for {fullfile}")

        if instrument == "ACIS":
            ###########################
            #
            # create ACIS ARF
            #
            ###########################

            try:
                if any([srcbkg == "src" and weight == "yes", srcbkg == "bkg" and dobkgresp]):
                    weight = "yes" # force background ARF to always be weighted
                    
                    ancrfile, weightfile, fef_file, tdetwmap = create_arf_ext(full_outroot,fullfile,asphist,
                                                                              ebin,clobber,verbose,
                                                                              phafile,da,msk,ewmap,
                                                                              bintwmap,wmap_clip,wmap_threshold,
                                                                              pars,tmpdir,infostr)

                    add_tool_history(ancrfile,toolname,pars,toolversion=__revision__)
                    
                if all([srcbkg == "src", weight == "no"]):
                    try:
                        ancrfile, weightfile = create_arf_ps(full_outroot,filename,fullfile,
                                                             asphist,ebin,clobber,verbose,
                                                             phafile,da,msk,
                                                             chip_id,skyx,skyy,chipx,chipy,infostr)

                        add_tool_history(ancrfile,toolname,pars,toolversion=__revision__)

                    except OSError:
                        raise IOError(f"Failed to create ARF for {fullfile}")

                    if correct == "yes":
                        v1(f"Calculating aperture correction for {srcbkg} ARF {iteminfostr}")

                        try:
                            ancrfile = correct_arf(full_outroot,fullfile,filename,
                                                   ancrfile,skyx,skyy,binarfcorr,clobber)
                        except OSError:
                            raise IOError(f"Failed to PSF correct the ARF: {ancrfile}")

            except (OSError,IOError):
                raise IOError(f"Failed to create ARF for {fullfile}")
            
            ###########################
            #
            # create ACIS RMF
            #
            ###########################

            infostr = f"Creating {srcbkg} RMF {iteminfostr}"

            if null_rmffile:
                v1(f"WARNING: Setting rmffile parameter (and calquiz calfile) to '{rmffile_ccd}'.\n")

            rmftool = determine_rmf_tool(phafile,rmffile_ccd,verbose)

            try:
                if any([srcbkg == "src", srcbkg == "bkg" and dobkgresp]):
                    if all([weight == "yes",weight_rmf == "yes"]):
                        respfile = build_rmf_ext(rmftool,ptype,full_outroot,
                                                 ebin,rmfbin,clobber,verbose,
                                                 phafile,weightfile,wmap_clip,
                                                 tdetwmap.name, infostr)

                        if wmap_clip:
                            tdetwmap.close()

                    else:
                        respfile = build_rmf_ps(rmftool,filename,fullfile,ptype,full_outroot,
                                                ebin,rmfbin,clobber,verbose,phafile,weightfile,
                                                chip_id,chipx,chipy,infostr)

                    try:
                        add_tool_history(respfile,toolname,pars,toolversion=__revision__)
                    except OSError:
                        v3("skip writing file history to RMF")

            except (OSError,IOError):
                raise IOError(f"Failed to create RMF for {fullfile}")

        else:
            ##########################################
            #
            # make HRC ARF and copy RMF from CalDB
            #
            ##########################################

            try:
                ancrfile, respfile = create_hrc_resp(phafile,rmffile,refcoord,full_outroot,
                                                     asphist,clobber,verbose,msk,
                                                     skyx,skyy,instrument,chip_id,infostr)

                add_tool_history(ancrfile,toolname,pars,toolversion=__revision__)

                try:
                    add_tool_history(respfile,toolname,pars,toolversion=__revision__)
                except OSError:
                    v3("skip writing file history to RMF")

                if all([srcbkg == "src",correct == "yes"]):
                    try:
                        v1(f"Calculating aperture correction for {srcbkg} ARF {iteminfostr}")

                        ancrfile = correct_arf(full_outroot,fullfile,filename,
                                               ancrfile,skyx,skyy,binarfcorr,clobber)
                    except OSError:
                        raise IOError(f"Failed to PSF correct the ARF: {ancrfile}")

            except (OSError,IOError):
                raise IOError(f"Failed to create ARF for {fullfile}")

        try:
            asphist_temp.close()
        except NameError:
            pass

        try:
            fef_file.close()
        except NameError:
            pass

        try:
            outreg.close()
        except NameError:
            pass

    if srcbkg == "bkg" and not dobkgresp:
        pass
    else:
        return {key_id : {"ARF" : ancrfile,
                          "RMF" : respfile}}


    
def groupspec(args):

    key_id = args["key_id"]
    srcbkg = args["srcbkg"]

    dogroup = args["dogroup"]
    bgdogroup = args["bgdogroup"]

    tmpdir = args["tmpdir"]
    clobber = args["clobber"]
    verbose = args["verbose"]
    iteminfostr = args["iteminfostr"]
    pars = args["pars_specextract"]

    full_outroot = args["full_outroot"]
    phafile = args["phafile"]
    ptype = args["ptype"]      

    try:
        gtype = args["gtype"]
    except KeyError:
        pass
    try:
        gval = args["gval"]
    except KeyError:
        pass
    try:
        binspec = args["binspec"]
    except KeyError:
        pass

    try:
        bggtype = args["bggtype"]
    except KeyError:
        pass
    try:
        bggval = args["bggval"]
    except KeyError:
        pass
    try:
        bgbinspec = args["bgbinspec"]
    except KeyError:
        pass

    if all([srcbkg == "src", dogroup]):
        v1(f"Grouping {srcbkg} spectrum {iteminfostr}")

        try:
            grpfile = group_spectrum(ptype,full_outroot,
                                     gval,binspec,gtype,
                                     clobber,verbose,phafile)

            return {key_id : {"grpfile" : grpfile}}

        except (OSError,IOError):
            raise IOError(f"Failed to group spectrum for {fullfile}")

    if all([srcbkg == "bkg", bgdogroup]):
        v1(f"Grouping {srcbkg} spectrum {iteminfostr}")

        try:
            grpfile = group_spectrum(ptype,full_outroot,
                                     bggval,bgbinspec,bggtype,
                                     clobber,verbose,phafile)
            return {key_id : {"grpfile" : grpfile}}

        except (OSError,IOError):
            raise IOError(f"Failed to group spectrum for {fullfile}")


        
def add_header_keys(args):
    key_id = args["key_id"]
    srcbkg = args["srcbkg"]
    phafile = args["phafile"]
    full_outroot = args["full_outroot"]    
    dobkgresp = args["dobkgresp"]
    bkgresp = args["bkgresp"]
    refcoord = args["refcoord"]

    dobg = args["dobg"]
    dogroup = args["dogroup"]
    bgdogroup = args["bgdogroup"]
    
    tmpdir = args["tmpdir"]
    verbose = args["verbose"]
    clobber = args["clobber"]

    iteminfostr = args["iteminfostr"]
    pars = args["pars_specextract"]

    try:
        ancrfile = args["ARF"]
    except KeyError:
        pass

    try:
        respfile = args["RMF"]
    except KeyError:
        pass
    
    try:
        grpfile = args["grpfile"]
    except KeyError:
        pass

    ####################################
    #
    # add header keys to output files
    #
    ####################################

    try:
        fn = phafile

        if srcbkg == "bkg" and not dobkgresp:
            if refcoord != "" and bkgresp == "yes":
                v1(f"Updating header of {fn} with RESPFILE and ANCRFILE keywords.\n")
                respfile = args["specdicts"][key_id.replace("bkg","src")]["RMF"]
                ancrfile = args["specdicts"][key_id.replace("bkg","src")]["ARF"]

                edit_headers(verbose, fn, "RESPFILE", os.path.basename(respfile))
                edit_headers(verbose, fn, "ANCRFILE", os.path.basename(ancrfile))

        else:
            v1(f"Updating header of {fn} with RESPFILE and ANCRFILE keywords.\n")

            edit_headers(verbose, fn, "RESPFILE", os.path.basename(respfile))
            edit_headers(verbose, fn, "ANCRFILE", os.path.basename(ancrfile))

            # If the source or background spectrum was grouped, add the respfile
            # and ancrfile keys there, too.
            if srcbkg == "src" and dogroup:
                fn = grpfile
                v1(f"Updating header of {fn} with RESPFILE and ANCRFILE keywords.\n")

                edit_headers(verbose, fn, "RESPFILE", os.path.basename(respfile))
                edit_headers(verbose, fn, "ANCRFILE", os.path.basename(ancrfile))

            if srcbkg == "bkg" and bgdogroup:
                fn = grpfile
                v1(f"Updating header of {fn} with RESPFILE and ANCRFILE keywords.\n")
                
                edit_headers(verbose, fn, "RESPFILE", os.path.basename(respfile))
                edit_headers(verbose, fn, "ANCRFILE", os.path.basename(ancrfile))

    except Exception:
        v1(f"Failed to update the RESPFILE and ANCRFILE keywords in {fn}.\n")

    try:
        if srcbkg == "src" and dobg:
            # Add the backfile key to the ungrouped source spectrum;
            # use the ungrouped background spectrum filename.

            fn = phafile
            backpha = args["specdicts"][key_id.replace("src","bkg")]["phafile"]

            v1(f"Updating header of {fn} with BACKFILE keyword.\n")
            edit_headers(verbose, fn, "BACKFILE", os.path.basename(backpha))

            # If the source spectrum was grouped, add the 
            # backfile key to the grouped source spectrum.

            if dogroup:
                # If the background is grouped, use the grouped background spectrum filename.
                if bgdogroup:
                    fn = grpfile
                    backphagrp = args["specdicts"][key_id.replace("src","bkg")]["grpfile"]

                    v1(f"Updating header of {fn} with BACKFILE keyword.\n")
                    edit_headers(verbose, fn, "BACKFILE", os.path.basename(backphagrp))

                else:
                    # If the background is not grouped, use the ungrouped background spectrum filename.
                    fn = grpfile
                    
                    v1(f"Updating header of {grpfile} with BACKFILE keyword.\n")
                    edit_headers(verbose, fn, "BACKFILE", os.path.basename(backpha))

    except Exception:
        v1(f"Failed to update the BACKFILE keyword in {fn}.\n")
                
    try:
        add_tool_history(phafile,toolname,pars,toolversion=__revision__)
    except Exception:
        v3(f"skip writing '{toolname}' history to {phafile}")

    try:
        add_tool_history(grpfile,toolname,pars,toolversion=__revision__)
    except OSError:
        v3(f"skip writing '{toolname}' history to {grpfile}")
    except NameError:
        pass
    

    
def coadd(outroot,spec_src_stk,spec_bkg_stk,arf_src_stk,arf_bkg_stk,rmf_src_stk,rmf_bkg_stk,dobg,dobkgresp,verbose,clobber):
    """
    Determine whether or not to combine source output spectra, and any
    associated background spectra and response files, if user has input
    a stack of source event files.
    """
    
    if len(spec_src_stk) < 2:
        v1("Warning: There are fewer than two source event files specified in the 'infile' parameter; the 'combine=yes' setting will be ignored.\n")

    else:
        #################################################
        #
        # Combine output spectra and responses if the
        # appropriate files were successfully created.
        #
        #################################################

        combine_spectra.punlearn()
        combine_spectra.outroot = f"{outroot}_combined"
        combine_spectra.clobber = clobber
        combine_spectra.verbose = verbose

        src_length = [len(spec_src_stk),len(arf_src_stk),len(rmf_src_stk)]

        if all(slen == src_length[0] for slen in src_length):
            combine_spectra.src_spectra = spec_src_stk
            combine_spectra.src_arfs = arf_src_stk
            combine_spectra.src_rmfs = rmf_src_stk

            combine_infostr = "Combined source spectra and responses."
            
            if dobg:
                if len(spec_bkg_stk) == len(spec_src_stk):
                    combine_spectra.bkg_spectra = spec_bkg_stk

                    combine_infostr = "Combined source spectra and responses, and background spectra."

                    if dobkgresp and all([arf_bkg_stk is not None,rmf_bkg_stk is not None]):
                        combine_spectra.bkg_arfs = arf_bkg_stk
                        combine_spectra.bkg_rmfs = rmf_bkg_stk
                        
                        combine_infostr = "Combined source and background spectra and responses."

            try:
                combine_spectra()
                v2(combine_infostr)

            except (OSError,IOError):
                v1("Failed to combine output spectra and responses, please do so manually or with 'combine_spectra'.")
                
        else:
            v1("Output spectra and responses were not combined because spectra and/or responses were not created for every item in the input stack(s) of files.")
            

            
##########################################################################
# 
#  Parameter related tasks
# 
#  Retrieve parameter values set in the referenced parameter file
#  and create a dictionary matching parameter name to parameter value.
##########################################################################

def get_par(args):
    """ Get specextract parameters from parameter file. """

    pinfo = open_param_file(args, toolname=toolname)
    pfile = pinfo["fp"]

    # Parameters:
    params = {}
    pars = {}

    pars["infile"] = params["infile"] = paramio.pgetstr(pfile, "infile")
    pars["outroot"] = params["outroot"] = paramio.pgetstr(pfile, "outroot")
    pars["weight"] = params["weight"] = paramio.pgetstr(pfile, "weight")
    pars["weight_rmf"] = params["weight_rmf"] = paramio.pgetstr(pfile, "weight_rmf")
    pars["correctpsf"] = params["correctpsf"] = paramio.pgetstr(pfile, "correctpsf")
    pars["combine"] = params["combine"] = paramio.pgetstr(pfile, "combine")
    pars["bkgfile"] = params["bkgfile"] = paramio.pgetstr(pfile, "bkgfile")
    pars["bkgresp"] = params["bkgresp"] = paramio.pgetstr(pfile, "bkgresp")
    pars["asp"] = params["asp"] = paramio.pgetstr(pfile, "asp")
    pars["refcoord"] = params["refcoord"] = paramio.pgetstr(pfile, "refcoord")
    pars["rmffile"] = params["rmffile"] = paramio.pgetstr(pfile, "rmffile")
    #pars["ptype"] = params["ptype"] = paramio.pgetstr(pfile, "ptype")
    pars["grouptype"] = params["gtype"] = paramio.pgetstr(pfile, "grouptype")
    pars["binspec"] = params["gspec"] = paramio.pgetstr(pfile, "binspec")
    pars["bkg_grouptype"] = params["bggtype"] = paramio.pgetstr(pfile, "bkg_grouptype")
    pars["bkg_binspec"] = params["bggspec"] = paramio.pgetstr(pfile, "bkg_binspec")
    pars["energy"] = params["ebin"] = paramio.pgetstr(pfile, "energy")
    pars["channel"] = params["channel"] = paramio.pgetstr(pfile, "channel")
    pars["energy_wmap"] = params["ewmap"] = paramio.pgetstr(pfile, "energy_wmap")
    pars["binarfwmap"] = params["binarfwmap"] = paramio.pgetstr(pfile, "binarfwmap")
    pars["binwmap"] = params["binwmap"] = paramio.pgetstr(pfile, "binwmap")
    pars["binarfcorr"] = params["binarfcorr"] = paramio.pgetstr(pfile, "binarfcorr")
    #pars["pbkfile"] = params["pbkfile"] = paramio.pgetstr(pfile, "pbkfile")
    pars["dtffile"] = params["dtffile"] = paramio.pgetstr(pfile, "dtffile")
    pars["mskfile"] = params["mskfile"] = paramio.pgetstr(pfile, "mskfile")
    pars["dafile"] = params["dafile"] = paramio.pgetstr(pfile, "dafile")
    pars["badpixfile"] = params["bpixfile"] = paramio.pgetstr(pfile, "badpixfile")
    pars["tmpdir"] = params["tmpdir"] = paramio.pgetstr(pfile,"tmpdir")
    pars["clobber"] = params["clobber"] = paramio.pgetstr(pfile, "clobber")
    pars["verbose"] = params["verbose"] = paramio.pgeti(pfile, "verbose")
    pars["mode"] = params["mode"] = paramio.pgetstr(pfile, "mode")

    #pars["wmap_clip"] = params["wmap_clip"] = paramio.pgetstr(pfile,"wmap_clip")
    #pars["wmap_threshold"] = params["wmap_threshold"] = paramio.pgetstr(pfile,"wmap_threshold")


    # verify the existence of the output directory, create if non-existent
    if params["outroot"].startswith("@"): 
        stackfile = stk.build(params["outroot"])
        out_roots = [st.replace(" ","").strip("\n") for st in stackfile]
        del(stackfile)
        
    else:
        out_roots = params["outroot"].replace(" ","").split(",")

    for out_root in out_roots:
        if out_root.endswith("/"):
            raise ValueError("outroot path must include a file root name and cannot be just a directory.")

        outdir,outhead = utils.split_outroot(out_root)

        if outdir != "":
            fileio.validate_outdir(outdir)

    ## uncomment when clipping implemented
    # if params["wmap_clip"].lower() == "yes":
    #     params["wmap_clip"] = True
    # else:
    #     params["wmap_clip"] = False
    params["wmap_clip"] = False # remove when clipping implemented
    params["wmap_threshold"] = None # remove when clipping implemented
        
    paramio.paramclose(pfile)

    return params,pars


def update_param_args_and_dict(dict_update,dict_orig,args_list,cmd_pars_history):
    """
    update the parameter dictionary (dict_orig) and argument list with new 
    keywords and revised entries defined in (dict_update)
    """

    for par in dict_update:
        if par is not None:
            for keyitem,keyitem_dict in par.items():
                for k,v in keyitem_dict.items():
                    dict_orig[keyitem][k] = v

    if not all(n is None for n in dict_update):
        return [{"key_id":key, **dict_orig[key], "pars_specextract" : cmd_pars_history} for key in dict_orig.keys()]
    else:
        return args_list


#######################################################################
#######################################################################
#####
#####  Create data products 
#####
#######################################################################
#######################################################################

@handle_ciao_errors(toolname, __revision__)
def run_specextract(args):
    
    #-----------------------------------------------------------
    # Retrieve parameter values from specextract parameter file.
    #-----------------------------------------------------------

    params,pars = get_par(args)

    #--------------------------------------------------
    # Check the input values and do some initial setup.
    #--------------------------------------------------

    # Set tool and module verbosity.

    set_verbosity(params["verbose"])
    utils.print_version(toolname, __revision__)
    v3(f"Parameters: {params}")

    # Define variables to represent parameter values.

    infile = params["infile"]
    outroot = params["outroot"]
    weight = params["weight"]
    weight_rmf = params["weight_rmf"]
    correct = params["correctpsf"]
    combine = params["combine"]
    bkgfile = params["bkgfile"]
    bkgresp = params["bkgresp"]
    asp = params["asp"]
    refcoord = params["refcoord"]
    rmffile = params["rmffile"]
    ptype = "PI" #params["ptype"]
    gtype = params["gtype"]
    gspec = params["gspec"]
    bggtype = params["bggtype"]
    bggspec = params["bggspec"]
    ebin = params["ebin"]
    channel = params["channel"]
    ewmap = params["ewmap"]
    binwmap = params["binwmap"]
    bintwmap = params["binarfwmap"]
    binarfcorr = params["binarfcorr"]
    dtffile = params["dtffile"]
    mask = params["mskfile"]
    dafile = params["dafile"]
    bpixfile = params["bpixfile"]
    tmpdir = params["tmpdir"]
    clobber = params["clobber"]
    verbose = params["verbose"]
    mode = params["mode"]

    wmap_clip = params["wmap_clip"]
    wmap_threshold = params["wmap_threshold"]

    # error out if there are spaces in absolute paths of the various parameters
    for parname in ["infile","outroot","bkgfile","asp","dtffile","mskfile","rmffile","bpixfile","dafile"]:
        if " " in os.path.abspath(params[parname]):
            raise IOError(f"The absolute path for the {parname}, '{os.path.abspath(params[parname])}', cannot contain any spaces")

    specdicts = spec.ParDicts(params).specextract_dict()  

    specextract_args = [{"key_id" : key, **specdicts[key], "pars_specextract" : pars} for key in specdicts.keys()]


    if len(specextract_args) > cpu_count():
        def parallelize(func,args):    
            return parallel_pool(func,args)
    else:
        def parallelize(func,args):
            return parallel_map(func,args)
        
    
    ###########################
    #
    # extract spectra
    #
    ###########################   

    pardict_update_spectra = parallelize(spectra,specextract_args)

    # update parameters dictionary if necessary, and the list passed to parallelization
    specextract_args = update_param_args_and_dict(pardict_update_spectra,specdicts,specextract_args,pars)

    ###########################
    #
    # generate responses
    #
    ###########################

    pardict_update_resps = parallelize(resps,specextract_args)
    specextract_args = update_param_args_and_dict(pardict_update_resps,specdicts,specextract_args,pars)

    ###########################
    #
    # optionally group spectrum
    #
    ###########################

    pardict_update_grp = parallelize(groupspec,specextract_args)
    specextract_args = update_param_args_and_dict(pardict_update_grp,specdicts,specextract_args,pars)
    
    ###########################
    #
    # update header keywords
    #
    ###########################

    specextract_args = [{**d , "specdicts" : specdicts} for d in specextract_args]
    parallelize(add_header_keys,specextract_args)

    # ## test in serial
    # for dargs in specextract_args:
    #     add_header_keys(dargs)
    
    ###########################
    #
    # co-add datasets
    #
    ###########################

    if combine == "yes":
        combine_outroot = specdicts["src1"]["full_outroot"].rstrip("_src1")
        dobg = specdicts["src1"]["dobg"]
        dobkgresp = specdicts["src1"]["dobkgresp"]

        sdvals = specdicts.values()

        src_spec = [d["phafile"] for d in sdvals if d["srcbkg"] is "src"]
        src_arf = [d["ARF"] for d in sdvals if d["srcbkg"] is "src"]
        src_rmf = [d["RMF"] for d in sdvals if d["srcbkg"] is "src"]
        
        if dobg:
            bkg_spec = [d["phafile"] for d in sdvals if d["srcbkg"] is "bkg"]

            if dobkgresp:
                bkg_arf = [d["ARF"] for d in sdvals if d["srcbkg"] is "bkg"]
                bkg_rmf = [d["RMF"] for d in sdvals if d["srcbkg"] is "bkg"]
            else:
                bkg_arf = None
                bkg_rmf = None
        else:
            bkg_spec = None
            bkg_arf = None
            bkg_rmf = None    
        
        coadd(combine_outroot,src_spec,bkg_spec,src_arf,bkg_arf,src_rmf,bkg_rmf,dobg,dobkgresp,verbose,clobber)


            
if __name__  == "__main__":
    try:
        run_specextract(sys.argv)
        # import timeit
        # runtime = timeit.timeit(lambda:run_specextract(sys.argv), number=10)
        # print(f"avg. runtime [s]: {runtime/10}")
    except Exception as E:
        print(f"\n# {toolname} ({__revision__}): ERROR {E} \n", file=sys.stderr)
        sys.exit(1)
    sys.exit(0)

    
